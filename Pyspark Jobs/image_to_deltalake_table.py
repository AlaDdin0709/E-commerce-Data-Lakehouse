from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta import DeltaTable
import json
from datetime import datetime

def create_spark_session():
    """CrÃ©er une session Spark avec les configurations nÃ©cessaires pour Delta Lake"""
    spark = SparkSession.builder \
        .appName("CustomerImagesJSONToDeltaLake") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio-api.minio.svc.cluster.local:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.kubernetes.file.upload.path", "/tmp") \
        .config("spark.kubernetes.executor.deleteOnTermination", "true") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark

def get_processed_files_path():
    """Obtenir le chemin du fichier qui stocke la liste des fichiers JSON images dÃ©jÃ  traitÃ©s"""
    return "s3a://customer-images/metadata/processed_json_files.json"

def ensure_metadata_bucket_exists(spark):
    """CrÃ©er le dossier metadata s'il n'existe pas"""
    try:
        test_df = spark.createDataFrame([("test",)], ["value"])
        test_path = "s3a://customer-images/metadata/_test_metadata_exists"
        test_df.write.mode("overwrite").json(test_path)
        test_df.limit(0).write.mode("overwrite").json(test_path)
        print("âœ… Dossier metadata customer-images existe et est accessible")
    except Exception as e:
        print(f"ğŸ“ CrÃ©ation du dossier metadata customer-images: {str(e)}")

def read_processed_files_list(spark):
    """Lire la liste des fichiers JSON images dÃ©jÃ  traitÃ©s"""
    processed_files_path = get_processed_files_path()
    
    try:
        schema = StructType([
            StructField("file_path", StringType(), True),
            StructField("processed_at", TimestampType(), True),
            StructField("record_count", LongType(), True)
        ])
        
        processed_files_df = spark.read.schema(schema).json(processed_files_path)
        
        if processed_files_df.count() > 0:
            processed_files = [row.file_path for row in processed_files_df.collect()]
            print(f"ğŸ“„ {len(processed_files)} fichiers JSON images dÃ©jÃ  traitÃ©s trouvÃ©s")
            return set(processed_files)
        else:
            print("ğŸ“„ Aucun fichier JSON images traitÃ© prÃ©cÃ©demment (premiÃ¨re exÃ©cution)")
            return set()
            
    except Exception as e:
        if "Path does not exist" in str(e) or "NoSuchKey" in str(e):
            print("ğŸ“„ Pas de fichier de mÃ©tadonnÃ©es JSON images (premiÃ¨re exÃ©cution)")
        else:
            print(f"âš ï¸ Erreur lors de la lecture des mÃ©tadonnÃ©es JSON images: {str(e)}")
        return set()

def save_processed_files_list(spark, new_files, record_counts):
    """Sauvegarder la liste mise Ã  jour des fichiers JSON images traitÃ©s"""
    if not new_files:
        print("â„¹ï¸ Aucun nouveau fichier JSON images Ã  ajouter aux mÃ©tadonnÃ©es")
        return
    
    try:
        print(f"ğŸ’¾ Sauvegarde de {len(new_files)} nouveaux fichiers JSON images dans les mÃ©tadonnÃ©es...")
        
        # Lire les fichiers dÃ©jÃ  traitÃ©s
        existing_files = read_processed_files_list(spark)
        
        # Combiner avec les nouveaux fichiers
        all_files = existing_files.union(new_files)
        
        # CrÃ©er le DataFrame avec les mÃ©tadonnÃ©es
        files_data = []
        for file_path in all_files:
            record_count = record_counts.get(file_path, 0)
            files_data.append({
                "file_path": file_path,
                "processed_at": datetime.now(),
                "record_count": record_count
            })
        
        files_df = spark.createDataFrame(files_data)
        
        processed_files_path = get_processed_files_path()
        
        files_df.coalesce(1).write \
               .mode("overwrite") \
               .json(processed_files_path)
        
        print(f"âœ… Liste des fichiers JSON images traitÃ©s mise Ã  jour: {len(all_files)} fichiers au total")
        
    except Exception as e:
        print(f"âŒ Erreur lors de la sauvegarde des mÃ©tadonnÃ©es JSON images: {str(e)}")
        import traceback
        traceback.print_exc()

def discover_json_files(spark):
    """DÃ©couvrir tous les fichiers JSON dans le dossier source customer images"""
    json_source_path = "s3a://customer-images/raw"
    
    try:
        print(f"ğŸ” Recherche des fichiers JSON images dans {json_source_path}...")
        
        json_files = []
        
        try:
            # Lecture rÃ©cursive pour dÃ©couvrir tous les fichiers JSON/JSON.GZ
            temp_df = spark.read.option("recursiveFileLookup", "true").text(json_source_path)
            input_files = temp_df.inputFiles()
            json_files = [f for f in input_files if (f.endswith('.json') or f.endswith('.json.gz')) and not f.endswith('_SUCCESS')]
            
            print(f"âœ… DÃ©couverte rÃ©ussie: {len(json_files)} fichiers JSON images trouvÃ©s")
            
        except Exception as recursive_error:
            print(f"âš ï¸ Erreur avec mÃ©thode rÃ©cursive: {str(recursive_error)}")
            print("ğŸ”„ Tentative avec lecture directe...")
            
            try:
                # Fallback: lecture directe
                temp_df = spark.read.text(json_source_path)
                input_files = temp_df.inputFiles()
                json_files = [f for f in input_files if (f.endswith('.json') or f.endswith('.json.gz')) and not f.endswith('_SUCCESS')]
                
                print(f"âœ… MÃ©thode directe rÃ©ussie: {len(json_files)} fichiers JSON images trouvÃ©s")
                
            except Exception as direct_error:
                print(f"âŒ Toutes les mÃ©thodes ont Ã©chouÃ©: {str(direct_error)}")
                return []
        
        # Afficher les fichiers trouvÃ©s
        if json_files:
            print(f"ğŸ“‚ {len(json_files)} fichiers JSON images dÃ©couverts:")
            for i, file in enumerate(json_files):
                file_type = "JSON compressÃ©" if file.endswith('.json.gz') else "JSON"
                print(f"  {i+1}. {file.split('/')[-1]} ({file_type})")
        else:
            print("âš ï¸ Aucun fichier JSON images trouvÃ©")
        
        return json_files
        
    except Exception as e:
        print(f"âŒ Erreur lors de la dÃ©couverte des fichiers JSON images: {str(e)}")
        import traceback
        traceback.print_exc()
        return []

def parse_customer_images_json(spark, json_files, processed_files):
    """Parser les fichiers JSON des images clients non traitÃ©s"""
    # Filtrer les fichiers dÃ©jÃ  traitÃ©s
    new_files = [f for f in json_files if f not in processed_files]
    
    if not new_files:
        print("âœ… Tous les fichiers JSON images ont dÃ©jÃ  Ã©tÃ© traitÃ©s")
        return None, set(), {}
    
    print(f"ğŸ“Š {len(new_files)} nouveaux fichiers JSON images Ã  traiter (ignorant {len(processed_files)} fichiers dÃ©jÃ  traitÃ©s):")
    for file in new_files:
        file_type = "JSON compressÃ©" if file.endswith('.json.gz') else "JSON"
        print(f"  - {file.split('/')[-1]} ({file_type})")
    
    try:
        combined_df = None
        record_counts = {}
        total_lines = 0
        
        # Lire fichier par fichier LIGNE PAR LIGNE
        for file_path in new_files:
            try:
                file_type = "JSON compressÃ©" if file_path.endswith('.json.gz') else "JSON"
                print(f"  ğŸ“„ Lecture ligne par ligne: {file_path.split('/')[-1]} ({file_type})")
                
                # Lire comme texte ligne par ligne
                if file_path.endswith('.json.gz'):
                    lines_df = spark.read \
                        .option("compression", "gzip") \
                        .text(file_path)
                else:
                    lines_df = spark.read.text(file_path)
                
                file_lines = lines_df.count()
                print(f"    ğŸ“Š {file_lines} lignes trouvÃ©es")
                
                if file_lines > 0:
                    # Ajouter le nom du fichier source
                    lines_df = lines_df.withColumn("source_file", lit(file_path))
                    
                    # Parser chaque ligne comme JSON Kafka
                    kafka_schema = StructType([
                        StructField("json_data", StringType(), True),
                        StructField("kafka_offset", LongType(), True),
                        StructField("kafka_partition", LongType(), True),
                        StructField("kafka_timestamp", StringType(), True),
                        StructField("processing_timestamp", StringType(), True)
                    ])
                    
                    # Parser les mÃ©tadonnÃ©es Kafka
                    kafka_df = lines_df.withColumn(
                        "kafka_parsed",
                        from_json(col("value"), kafka_schema)
                    ).select(
                        col("source_file"),
                        col("kafka_parsed.*")
                    ).filter(col("json_data").isNotNull())
                    
                    kafka_count = kafka_df.count()
                    print(f"    âœ… {kafka_count} enregistrements Kafka images parsÃ©s")
                    
                    if kafka_count > 0:
                        if combined_df is None:
                            combined_df = kafka_df
                        else:
                            combined_df = combined_df.union(kafka_df)
                        
                        total_lines += kafka_count
                        record_counts[file_path] = kafka_count
                    else:
                        record_counts[file_path] = 0
                else:
                    print(f"    âš ï¸ Fichier vide ignorÃ©: {file_path}")
                    record_counts[file_path] = 0
                    
            except Exception as file_error:
                print(f"    âŒ Erreur lors de la lecture de {file_path}: {str(file_error)}")
                record_counts[file_path] = 0
                continue
        
        if combined_df is not None and total_lines > 0:
            print(f"âœ… Total des enregistrements Kafka images collectÃ©s: {total_lines}")
            
            # Parser le JSON imbriquÃ© des donnÃ©es customer images
            print("ğŸ” Parsing du JSON customer images imbriquÃ©...")
            
            # SchÃ©ma pour les donnÃ©es customer images imbriquÃ©es (basÃ© sur le diagnostic)
            customer_images_schema = StructType([
                StructField("image_id", StringType(), True),
                StructField("s3_path", StringType(), True),
                StructField("upload_timestamp", StringType(), True),
                StructField("customer_id", StringType(), True),
                StructField("order_id", StringType(), True),
                StructField("image_type", StringType(), True),
                StructField("file_size", LongType(), True),
                StructField("image_format", StringType(), True),
                StructField("width", IntegerType(), True),
                StructField("height", IntegerType(), True),
                StructField("quality_score", DoubleType(), True),
                StructField("processing_status", StringType(), True),
                StructField("metadata", MapType(StringType(), StringType()), True)
            ])
            
            # Parser le JSON imbriquÃ© des images clients
            final_df = combined_df.withColumn(
                "customer_images_parsed",
                from_json(col("json_data"), customer_images_schema)
            ).select(
                col("source_file"),
                col("kafka_offset"),
                col("kafka_partition"),
                col("kafka_timestamp"),
                col("processing_timestamp"),
                col("json_data").alias("raw_json_data"),
                # Extraire les champs customer images
                col("customer_images_parsed.image_id").alias("image_id"),
                col("customer_images_parsed.s3_path").alias("s3_path"),
                col("customer_images_parsed.upload_timestamp").alias("upload_timestamp"),
                col("customer_images_parsed.customer_id").alias("customer_id"),
                col("customer_images_parsed.order_id").alias("order_id"),
                col("customer_images_parsed.image_type").alias("image_type"),
                col("customer_images_parsed.file_size").alias("file_size"),
                col("customer_images_parsed.image_format").alias("image_format"),
                col("customer_images_parsed.width").alias("width"),
                col("customer_images_parsed.height").alias("height"),
                col("customer_images_parsed.quality_score").alias("quality_score"),
                col("customer_images_parsed.processing_status").alias("processing_status"),
                col("customer_images_parsed.metadata").alias("image_metadata")
            )
            
            # Convertir les timestamps
            final_df = final_df \
                .withColumn("kafka_timestamp", to_timestamp(col("kafka_timestamp"))) \
                .withColumn("processing_timestamp", to_timestamp(col("processing_timestamp"))) \
                .withColumn("upload_timestamp", to_timestamp(col("upload_timestamp")))
            
            # Extraire l'ordre de la colonne order_id ou du s3_path
            final_df = final_df.withColumn(
                "extracted_order_id",
                when(col("order_id").isNotNull(), col("order_id"))
                .otherwise(
                    regexp_extract(col("s3_path"), r"order_(\d+)_", 1)
                )
            )
            
            # Extraire les dimensions de l'image si disponibles
            final_df = final_df.withColumn(
                "image_size_mb",
                when(col("file_size").isNotNull(), round(col("file_size") / 1024 / 1024, 2))
                .otherwise(lit(None))
            )
            
            # Ajouter les mÃ©tadonnÃ©es de partitionnement pour Delta Lake
            final_df = final_df.withColumn("delta_load_timestamp", current_timestamp())
            
            # GÃ©rer le partitionnement basÃ© sur upload_timestamp ou processing_timestamp
            if "upload_timestamp" in final_df.columns:
                final_df = final_df.withColumn("partition_year", year(col("upload_timestamp"))) \
                                 .withColumn("partition_month", month(col("upload_timestamp"))) \
                                 .withColumn("partition_day", dayofmonth(col("upload_timestamp")))
            elif "processing_timestamp" in final_df.columns:
                final_df = final_df.withColumn("partition_year", year(col("processing_timestamp"))) \
                                 .withColumn("partition_month", month(col("processing_timestamp"))) \
                                 .withColumn("partition_day", dayofmonth(col("processing_timestamp")))
            else:
                # Fallback Ã  la date actuelle
                final_df = final_df.withColumn("partition_year", year(current_timestamp())) \
                                 .withColumn("partition_month", month(current_timestamp())) \
                                 .withColumn("partition_day", dayofmonth(current_timestamp()))
            
            # Filtre pour conserver les donnÃ©es d'images valides
            final_df = final_df.filter(
                (col("image_id").isNotNull()) | 
                (col("s3_path").isNotNull()) |
                (col("customer_id").isNotNull()) |
                (col("extracted_order_id").isNotNull())
            )
            
            # S'assurer que les partitions ne sont pas NULL
            final_df = final_df.filter(
                col("partition_year").isNotNull() & 
                col("partition_month").isNotNull() & 
                col("partition_day").isNotNull()
            )
            
            final_count = final_df.count()
            print(f"âœ… Enregistrements images finaux aprÃ¨s traitement: {final_count}")
            
            # Afficher un Ã©chantillon des donnÃ©es d'images
            print(f"\nğŸ–¼ï¸ Ã‰chantillon des donnÃ©es customer images:")
            final_df.select("image_id", "s3_path", "customer_id", "extracted_order_id", "image_type", "file_size") \
                   .filter(col("image_id").isNotNull()) \
                   .show(5, truncate=False)
            
            return final_df, set(new_files), record_counts
        else:
            print("âš ï¸ Aucun enregistrement d'images valide trouvÃ© dans les nouveaux fichiers JSON")
            return None, set(new_files), record_counts
            
    except Exception as e:
        print(f"âŒ Erreur lors du parsing des fichiers JSON images: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, set(), {}

def create_or_update_customer_images_delta_table(spark, df, delta_path):
    """CrÃ©er ou mettre Ã  jour la table Delta Lake pour customer images"""
    if df is None:
        print("âš ï¸ Aucune donnÃ©e customer images Ã  Ã©crire dans Delta Lake")
        return
    
    print(f"ğŸ”„ Traitement de {df.count()} nouveaux enregistrements customer images pour Delta Lake...")
    
    try:
        # Validation des colonnes de partitionnement
        print("ğŸ” Validation des colonnes de partitionnement...")
        
        partition_columns = ["partition_year", "partition_month", "partition_day"]
        for col_name in partition_columns:
            if col_name not in df.columns:
                print(f"âŒ Colonne de partitionnement manquante: {col_name}")
                return
            
            null_count = df.filter(col(col_name).isNull()).count()
            if null_count > 0:
                print(f"âŒ {null_count} valeurs NULL trouvÃ©es dans {col_name}")
                return
        
        # VÃ©rifier si la table Delta existe et utiliser APPEND si elle existe
        if DeltaTable.isDeltaTable(spark, delta_path):
            print("ğŸ“Š Table Delta customer images existante trouvÃ©e - Mode APPEND")
            
            # Utiliser APPEND pour ajouter de nouvelles donnÃ©es
            df.write \
              .format("delta") \
              .mode("append") \
              .partitionBy("partition_year", "partition_month", "partition_day") \
              .option("mergeSchema", "true") \
              .option("delta.autoOptimize.optimizeWrite", "true") \
              .option("delta.autoOptimize.autoCompact", "true") \
              .save(delta_path)
            
            print(f"âœ… Nouveaux enregistrements images ajoutÃ©s Ã  la table existante")
            
        else:
            print("ğŸ†• CrÃ©ation d'une nouvelle table Delta Lake customer images")
            
            df.write \
              .format("delta") \
              .mode("overwrite") \
              .partitionBy("partition_year", "partition_month", "partition_day") \
              .option("delta.autoOptimize.optimizeWrite", "true") \
              .option("delta.autoOptimize.autoCompact", "true") \
              .save(delta_path)
            
            print(f"âœ… Table Delta customer images crÃ©Ã©e avec {df.count()} enregistrements")
        
        # Afficher des statistiques des images
        print("\n=== STATISTIQUES DE LA TABLE DELTA CUSTOMER IMAGES ===")
        try:
            delta_table = DeltaTable.forPath(spark, delta_path)
            final_df = delta_table.toDF()
            
            total_count = final_df.count()
            print(f"ğŸ“Š Total des enregistrements d'images dans la table: {total_count}")
            
            # Structure des partitions
            print("\nğŸ“ Structure des partitions:")
            partition_info = final_df.groupBy("partition_year", "partition_month", "partition_day").count() \
                                    .orderBy("partition_year", "partition_month", "partition_day")
            partition_info.show()
            
            # Statistiques par type d'image
            if "image_type" in final_df.columns:
                print("\nğŸ–¼ï¸ RÃ©partition par type d'image:")
                final_df.groupBy("image_type").count().orderBy(desc("count")).show()
            
            # Statistiques par format d'image
            if "image_format" in final_df.columns:
                print("\nğŸ“¸ RÃ©partition par format d'image:")
                final_df.groupBy("image_format").count().orderBy(desc("count")).show()
            
            # Statistiques des clients et commandes
            if "customer_id" in final_df.columns:
                print("\nğŸ‘¥ Nombre de clients uniques:")
                customer_count = final_df.select("customer_id").filter(col("customer_id").isNotNull()).distinct().count()
                print(f"    Clients uniques: {customer_count}")
            
            if "extracted_order_id" in final_df.columns:
                print("\nğŸ“¦ Nombre de commandes uniques:")
                order_count = final_df.select("extracted_order_id").filter(col("extracted_order_id").isNotNull()).distinct().count()
                print(f"    Commandes uniques: {order_count}")
            
            # Analyse des tailles de fichiers
            if "file_size" in final_df.columns:
                print("\nğŸ“ Statistiques des tailles de fichiers:")
                size_stats = final_df.select("file_size", "image_size_mb").filter(col("file_size").isNotNull())
                if size_stats.count() > 0:
                    size_stats.describe().show()
            
            # Analyse des dimensions d'images
            if "width" in final_df.columns and "height" in final_df.columns:
                print("\nğŸ“ Statistiques des dimensions d'images:")
                dimension_stats = final_df.select("width", "height").filter(col("width").isNotNull() & col("height").isNotNull())
                if dimension_stats.count() > 0:
                    dimension_stats.describe().show()
            
            # VÃ©rifier les donnÃ©es rÃ©centes
            print("\nğŸ“Š Ã‰chantillon des images rÃ©centes:")
            recent_data = final_df.select("image_id", "s3_path", "customer_id", "extracted_order_id", "image_format", "upload_timestamp") \
                                 .filter(col("image_id").isNotNull()) \
                                 .orderBy(desc("upload_timestamp")) \
                                 .limit(5)
            recent_data.show(truncate=False)
            
        except Exception as stats_error:
            print(f"âš ï¸ Impossible d'afficher les statistiques images: {str(stats_error)}")
        
        # Optimiser la table
        print("ğŸ”§ Optimisation de la table Delta customer images...")
        try:
            spark.sql(f"OPTIMIZE delta.`{delta_path}`")
            print("âœ… Table Delta customer images optimisÃ©e")
        except Exception as opt_error:
            print(f"âš ï¸ Optimisation Ã©chouÃ©e: {str(opt_error)}")
        
        print(f"\nğŸ“‹ Table Delta customer images disponible Ã : {delta_path}")
        
    except Exception as e:
        print(f"âŒ Erreur lors de la crÃ©ation/mise Ã  jour de la table Delta customer images: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e

def main():
    """Fonction principale pour convertir les JSON customer images en Delta Lake"""
    print("=== CONVERSION JSON CUSTOMER IMAGES VERS DELTA LAKE ===")
    
    spark = create_spark_session()
    
    try:
        # S'assurer que le dossier metadata existe
        ensure_metadata_bucket_exists(spark)
        
        # DÃ©couvrir tous les fichiers JSON images
        json_files = discover_json_files(spark)
        
        if not json_files:
            print("âš ï¸ Aucun fichier JSON customer images trouvÃ© Ã  traiter")
            return
        
        # Lire les fichiers dÃ©jÃ  traitÃ©s
        processed_files = read_processed_files_list(spark)
        print(f"ğŸ“‹ Fichiers images dÃ©jÃ  traitÃ©s: {len(processed_files)}")
        
        # Parser SEULEMENT les nouveaux fichiers JSON images
        new_data_df, new_files_processed, record_counts = parse_customer_images_json(spark, json_files, processed_files)
        
        if new_data_df is not None and new_data_df.count() > 0:
            # Chemin de la table Delta Lake customer images
            delta_table_path = "s3a://customer-images/bronze"
            
            # CrÃ©er ou mettre Ã  jour la table Delta avec APPEND
            create_or_update_customer_images_delta_table(spark, new_data_df, delta_table_path)
            
            # Sauvegarder les mÃ©tadonnÃ©es SEULEMENT aprÃ¨s succÃ¨s
            if new_files_processed:
                print("ğŸ’¾ Sauvegarde des nouveaux fichiers images traitÃ©s dans les mÃ©tadonnÃ©es...")
                save_processed_files_list(spark, new_files_processed, record_counts)
            
            print(f"âœ… Conversion customer images terminÃ©e avec succÃ¨s! {new_data_df.count()} nouveaux enregistrements ajoutÃ©s Ã  Delta Lake.")
            print(f"ğŸ“ Table Delta Lake customer images disponible Ã : {delta_table_path}")
            
        else:
            print("â„¹ï¸ Aucune nouvelle donnÃ©e customer images Ã  traiter - tous les fichiers sont dÃ©jÃ  traitÃ©s")
            
    except Exception as e:
        print(f"âŒ Erreur lors de la conversion customer images: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e
    finally:
        print("ğŸ”š Fermeture de la session Spark...")
        spark.stop()

if __name__ == "__main__":
    main()

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import csv
import io
import json
import os

def create_spark_session():
    """Cr√©er une session Spark avec les configurations n√©cessaires"""
    spark = SparkSession.builder \
        .appName("KafkaToMinIOMultiTopicConsumer") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio-api.minio.svc.cluster.local:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.kubernetes.file.upload.path", "/tmp") \
        .config("spark.kubernetes.executor.deleteOnTermination", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark

def get_offset_file_path(topic_name):
    """Obtenir le chemin du fichier d'offset pour un topic"""
    return f"s3a://kafka-offsets/{topic_name}_last_offset.json"

def ensure_offset_bucket_exists(spark):
    """Cr√©er le bucket kafka-offsets s'il n'existe pas"""
    try:
        # Tester si le bucket existe en essayant d'√©crire un fichier test
        test_df = spark.createDataFrame([("test",)], ["value"])
        test_path = "s3a://kafka-offsets/_test_bucket_exists"
        test_df.write.mode("overwrite").json(test_path)
        
        # Supprimer le fichier test
        test_df.limit(0).write.mode("overwrite").json(test_path)
        print("‚úÖ Bucket kafka-offsets existe et est accessible")
    except Exception as e:
        if "NoSuchBucket" in str(e) or "does not exist" in str(e):
            print("üìÅ Cr√©ation du bucket kafka-offsets...")
            try:
                # Cr√©er le bucket en √©crivant un fichier temporaire puis le supprimer
                temp_df = spark.createDataFrame([("init",)], ["temp"])
                temp_df.write.mode("overwrite").json("s3a://kafka-offsets/temp_init")
                
                # Supprimer le fichier temporaire
                temp_df.limit(0).write.mode("overwrite").json("s3a://kafka-offsets/temp_init")
                print("‚úÖ Bucket kafka-offsets cr√©√© avec succ√®s")
            except Exception as create_error:
                print(f"‚ùå Impossible de cr√©er le bucket kafka-offsets: {str(create_error)}")
                print("üîß Veuillez cr√©er manuellement le bucket 'kafka-offsets' dans MinIO")
        else:
            print(f"‚ö†Ô∏è Erreur lors de la v√©rification du bucket: {str(e)}")

def read_last_offsets(spark, topic_name):
    """Lire les derniers offsets sauvegard√©s depuis MinIO"""
    offset_path = get_offset_file_path(topic_name)
    
    try:
        # Essayer de lire le fichier d'offset avec un sch√©ma d√©fini
        from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType
        
        offset_schema = StructType([
            StructField("partition", LongType(), True),
            StructField("offset", LongType(), True),
            StructField("topic", StringType(), True),
            StructField("saved_at", TimestampType(), True)
        ])
        
        offset_df = spark.read.schema(offset_schema).json(offset_path)
        
        if offset_df.count() > 0:
            # Convertir en format JSON pour Kafka - AVEC le nom du topic
            offsets_by_partition = {}
            rows = offset_df.collect()
            for row in rows:
                partition = str(row.partition)
                offset = row.offset + 1  # +1 pour commencer apr√®s le dernier message trait√©
                offsets_by_partition[partition] = offset
            
            # Format JSON pour Kafka - CORRECT avec nom du topic
            kafka_offsets = json.dumps({topic_name: offsets_by_partition})
            print(f"Offsets trouv√©s pour {topic_name}: {kafka_offsets}")
            return kafka_offsets
        else:
            print(f"Aucun offset trouv√© pour {topic_name}, d√©marrage depuis le d√©but")
            return "earliest"
    except Exception as e:
        if "NoSuchBucket" in str(e):
            print(f"‚ö†Ô∏è Bucket kafka-offsets n'existe pas. D√©marrage depuis le d√©but pour {topic_name}")
        elif "Path does not exist" in str(e) or "NoSuchKey" in str(e):
            print(f"üìÑ Pas de fichier d'offset pour {topic_name} (premi√®re ex√©cution). D√©marrage depuis le d√©but")
        else:
            print(f"Impossible de lire les offsets pour {topic_name}: {str(e)}")
        return "earliest"

def save_last_offsets(spark, df, topic_name):
    """Sauvegarder les derniers offsets trait√©s vers MinIO"""
    if df is None or df.count() == 0:
        print(f"Aucun offset √† sauvegarder pour {topic_name}")
        return
    
    try:
        # Obtenir les offsets maximum par partition
        max_offsets = df.groupBy("kafka_partition") \
                       .agg(max("kafka_offset").alias("max_offset")) \
                       .select(
                           col("kafka_partition").alias("partition"),
                           col("max_offset").alias("offset"),
                           lit(topic_name).alias("topic"),
                           current_timestamp().alias("saved_at")
                       )
        
        offset_path = get_offset_file_path(topic_name)
        
        # Sauvegarder les offsets (remplacer le fichier existant)
        max_offsets.coalesce(1).write \
                   .mode("overwrite") \
                   .json(offset_path)
        
        print(f"Offsets sauvegard√©s pour {topic_name}:")
        max_offsets.show()
        
    except Exception as e:
        print(f"Erreur lors de la sauvegarde des offsets pour {topic_name}: {str(e)}")

def parse_kafka_csv_message(csv_message):
    """
    Parser sp√©cialis√© pour les messages Kafka qui contiennent :
    - L'en-t√™te CSV sur la premi√®re ligne
    - Les donn√©es r√©elles sur les lignes suivantes
    - S√©par√©s par \n dans le m√™me message
    """
    if csv_message is None or csv_message.strip() == "":
        return None
    
    try:
        # Diviser le message par les retours √† la ligne
        lines = csv_message.strip().split('\n')
        
        if len(lines) < 2:
            # Si on a qu'une seule ligne, c'est probablement juste un en-t√™te
            return None
        
        # La premi√®re ligne est l'en-t√™te, les suivantes sont les donn√©es
        header_line = lines[0]
        data_lines = lines[1:]
        
        # V√©rifier que la premi√®re ligne est bien un en-t√™te
        if not header_line.startswith('order_id,customer_id'):
            # Si ce n'est pas un en-t√™te reconnu, traiter comme une ligne de donn√©es normale
            reader = csv.reader(io.StringIO(csv_message))
            return next(reader)
        
        # Parser chaque ligne de donn√©es (ignorer l'en-t√™te)
        parsed_rows = []
        for data_line in data_lines:
            if data_line.strip():  # Ignorer les lignes vides
                try:
                    reader = csv.reader(io.StringIO(data_line))
                    row = next(reader)
                    parsed_rows.append(row)
                except Exception as e:
                    print(f"Erreur parsing ligne: {data_line}, erreur: {str(e)}")
                    continue
        
        # Retourner la premi√®re ligne de donn√©es valide
        return parsed_rows[0] if parsed_rows else None
        
    except Exception as e:
        print(f"Erreur parsing message Kafka: {csv_message[:100]}..., erreur: {str(e)}")
        return None

def validate_transaction_row(parsed_row):
    """Valider qu'une ligne de transaction contient des donn√©es valides"""
    if parsed_row is None or len(parsed_row) < 14:
        return False
    
    # V√©rifications de base
    order_id = parsed_row[0] if len(parsed_row) > 0 else ""
    customer_id = parsed_row[1] if len(parsed_row) > 1 else ""
    amount_str = parsed_row[7] if len(parsed_row) > 7 else ""
    
    # L'order_id ne doit pas √™tre vide ou √™tre le nom de la colonne
    if not order_id or order_id == "order_id" or order_id.strip() == "":
        return False
    
    # Le customer_id ne doit pas √™tre vide ou √™tre le nom de la colonne
    if not customer_id or customer_id == "customer_id" or customer_id.strip() == "":
        return False
    
    # Le montant doit √™tre convertible en nombre
    if amount_str == "amount" or amount_str == "NULL" or not amount_str or amount_str.strip() == "":
        return False
    
    try:
        amount_value = float(amount_str)
        # V√©rifier que le montant est raisonnable (pas n√©gatif, pas trop grand)
        if amount_value < 0 or amount_value > 100000:
            return False
    except (ValueError, TypeError):
        return False
    
    # V√©rifier que l'order_id ressemble √† un UUID ou ID valide
    if len(order_id) < 10:  # IDs trop courts sont suspects
        return False
    
    return True

def process_transactions_topic_fixed(spark):
    """Traitement du topic transactions avec parser sp√©cialis√© pour le format Kafka"""
    print("=== TRAITEMENT DU TOPIC TRANSACTIONS (FORMAT KAFKA SP√âCIALIS√â) ===")
    
    # Lire les derniers offsets
    starting_offsets = read_last_offsets(spark, "transactions")
    
    # Lire depuis Kafka avec les offsets appropri√©s
    df = spark.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "172.17.34.127:9092") \
        .option("subscribe", "transactions") \
        .option("startingOffsets", starting_offsets) \
        .option("endingOffsets", "latest") \
        .option("failOnDataLoss", "false") \
        .load()
    
    print(f"Nombre total de messages lus depuis Kafka (transactions): {df.count()}")
    
    if df.count() == 0:
        print("Aucun nouveau message trouv√© dans Kafka pour transactions.")
        return None
    
    # Convertir la valeur Kafka en string
    kafka_df = df.select(
        col("key").cast("string").alias("key"),
        col("value").cast("string").alias("csv_data"),
        col("timestamp").alias("kafka_timestamp"),
        col("partition").alias("kafka_partition"),
        col("offset").alias("kafka_offset")
    )
    
    # Filtrer les valeurs nulles ou vides
    kafka_df = kafka_df.filter(col("csv_data").isNotNull() & (col("csv_data") != ""))
    
    print(f"Nombre de messages valides apr√®s filtrage: {kafka_df.count()}")
    
    if kafka_df.count() == 0:
        return None
    
    # D√âBOGAGE: Afficher quelques exemples de donn√©es brutes
    print("=== EXEMPLES DE DONN√âES BRUTES KAFKA ===")
    kafka_df.select("csv_data").show(3, truncate=False)
    
    # UDF pour parser les messages Kafka sp√©cialis√©s
    parse_kafka_csv_udf = udf(parse_kafka_csv_message, ArrayType(StringType()))
    
    # Parser les donn√©es avec le nouveau parser
    parsed_df = kafka_df.withColumn("parsed_data", parse_kafka_csv_udf(col("csv_data")))
    
    # Filtrer les lignes o√π le parsing a √©chou√© ou retourn√© None
    parsed_df = parsed_df.filter(col("parsed_data").isNotNull())
    
    # V√©rifier que nous avons assez de colonnes (14 colonnes attendues)
    parsed_df = parsed_df.filter(size(col("parsed_data")) >= 14)
    
    print(f"Nombre de messages apr√®s parsing CSV sp√©cialis√©: {parsed_df.count()}")
    
    if parsed_df.count() == 0:
        print("‚ö†Ô∏è AUCUNE DONN√âE VALIDE APR√àS PARSING SP√âCIALIS√â!")
        return None
    
    # D√âBOGAGE: Afficher la structure des donn√©es pars√©es
    print("=== EXEMPLES DE DONN√âES PARS√âES (NOUVELLES) ===")
    parsed_df.select("parsed_data").show(5, truncate=False)
    
    # Validation suppl√©mentaire des donn√©es
    validate_row_udf = udf(validate_transaction_row, BooleanType())
    with_validation = parsed_df.withColumn("is_valid_transaction", validate_row_udf(col("parsed_data")))
    
    invalid_count = with_validation.filter(col("is_valid_transaction") == False).count()
    valid_count = with_validation.filter(col("is_valid_transaction") == True).count()
    
    print(f"=== VALIDATION DES DONN√âES ===")
    print(f"Lignes valides: {valid_count}")
    print(f"Lignes invalides: {invalid_count}")
    
    if valid_count == 0:
        print("‚ö†Ô∏è AUCUNE TRANSACTION VALIDE TROUV√âE APR√àS VALIDATION!")
        return None
    
    # Garder seulement les transactions valides
    valid_transactions = with_validation.filter(col("is_valid_transaction") == True)
    
    # Extraire les colonnes individuelles
    transactions_df = valid_transactions.select(
        col("parsed_data")[0].alias("order_id"),
        col("parsed_data")[1].alias("customer_id"),
        col("parsed_data")[2].alias("customer_first_name"),
        col("parsed_data")[3].alias("customer_last_name"),
        col("parsed_data")[4].alias("product_id"),
        col("parsed_data")[5].alias("product_name"),
        col("parsed_data")[6].alias("category"),
        col("parsed_data")[7].cast("double").alias("amount"),
        col("parsed_data")[8].alias("payment_method"),
        col("parsed_data")[9].alias("payment_status"),
        col("parsed_data")[10].alias("discount_code"),
        col("parsed_data")[11].alias("shipping_address"),
        col("parsed_data")[12].alias("timestamp_raw"),  # Garder en string d'abord
        when(col("parsed_data")[13].isin("True", "true", "TRUE", "1"), True)
        .when(col("parsed_data")[13].isin("False", "false", "FALSE", "0"), False)
        .otherwise(None).alias("is_returned"),
        col("kafka_timestamp"),
        col("kafka_partition"),
        col("kafka_offset")
    )
    
    # D√âBOGAGE: V√©rifier les donn√©es extraites
    print("=== DONN√âES EXTRAITES AVANT CONVERSION TIMESTAMP ===")
    print(f"Nombre d'enregistrements: {transactions_df.count()}")
    transactions_df.select("order_id", "customer_id", "amount", "timestamp_raw", "payment_status").show(5, truncate=False)
    
    # Conversion des timestamps
    print("=== CONVERSION DES TIMESTAMPS ===")
    
    timestamp_df = transactions_df.withColumn(
        "timestamp",
        coalesce(
            to_timestamp(col("timestamp_raw"), "yyyy-MM-dd'T'HH:mm:ss"),
            to_timestamp(col("timestamp_raw"), "yyyy-MM-dd HH:mm:ss"),
            to_timestamp(col("timestamp_raw"), "yyyy-MM-dd'T'HH:mm:ss.SSS"),
            to_timestamp(col("timestamp_raw"), "dd/MM/yyyy HH:mm:ss"),
            to_timestamp(col("timestamp_raw"), "MM/dd/yyyy HH:mm:ss"),
            to_timestamp(col("timestamp_raw")),  # Format par d√©faut
        )
    )
    
    # V√©rifier la conversion des timestamps
    valid_timestamps = timestamp_df.filter(col("timestamp").isNotNull()).count()
    total_records = timestamp_df.count()
    
    print(f"Timestamps convertis avec succ√®s: {valid_timestamps}/{total_records}")
    
    if valid_timestamps == 0:
        print("‚ö†Ô∏è AUCUN TIMESTAMP CONVERTI - UTILISATION DE LA DATE ACTUELLE")
        final_df = timestamp_df.withColumn("timestamp", current_timestamp())
    else:
        final_df = timestamp_df.filter(col("timestamp").isNotNull())
        print(f"‚úÖ {valid_timestamps} timestamps convertis avec succ√®s!")
    
    # Ajouter des colonnes de partitioning
    final_df = final_df.withColumn("year", year(col("timestamp"))) \
                       .withColumn("month", month(col("timestamp"))) \
                       .withColumn("day", dayofmonth(col("timestamp"))) \
                       .withColumn("processing_date", current_date())
    
    # Supprimer la colonne timestamp_raw temporaire
    final_df = final_df.drop("timestamp_raw")
    
    print(f"‚úÖ Nombre final d'enregistrements √† traiter: {final_df.count()}")
    
    # D√âBOGAGE: Afficher quelques enregistrements finaux
    if final_df.count() > 0:
        print("=== EXEMPLES D'ENREGISTREMENTS FINAUX ===")
        final_df.select("order_id", "customer_id", "amount", "timestamp", "payment_status", "category").show(10)
        
        # Statistiques rapides
        print("=== STATISTIQUES RAPIDES ===")
        print("Montants (min/max/avg):")
        final_df.select(
            min("amount").alias("min_amount"), 
            max("amount").alias("max_amount"),
            avg("amount").alias("avg_amount")
        ).show()
        
        print("Top 5 cat√©gories:")
        final_df.groupBy("category").count().orderBy(desc("count")).show(5)
        
        print("Statuts de paiement:")
        final_df.groupBy("payment_status").count().show()
    
    return final_df

def process_json_topic(spark, topic_name, destination_bucket):
    """Traitement g√©n√©rique pour les topics JSON avec gestion des offsets"""
    print(f"=== TRAITEMENT DU TOPIC {topic_name.upper()} ===")
    
    # Lire les derniers offsets
    starting_offsets = read_last_offsets(spark, topic_name)
    
    # Lire depuis Kafka avec les offsets appropri√©s
    df = spark.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "172.17.34.127:9092") \
        .option("subscribe", topic_name) \
        .option("startingOffsets", starting_offsets) \
        .option("endingOffsets", "latest") \
        .option("failOnDataLoss", "false") \
        .load()
    
    print(f"Nombre total de messages lus depuis Kafka ({topic_name}): {df.count()}")
    
    if df.count() == 0:
        print(f"Aucun nouveau message trouv√© dans Kafka pour {topic_name}.")
        return None
    
    # Convertir la valeur en string et ajouter m√©tadonn√©es
    json_df = df.select(
        col("key").cast("string").alias("kafka_key"),
        col("value").cast("string").alias("json_data"),
        col("timestamp").alias("kafka_timestamp"),
        col("partition").alias("kafka_partition"),
        col("offset").alias("kafka_offset"),
        current_timestamp().alias("processing_timestamp")
    )
    
    # Filtrer les valeurs nulles ou vides
    json_df = json_df.filter(col("json_data").isNotNull() & (col("json_data") != ""))
    
    print(f"Nombre de messages valides apr√®s filtrage ({topic_name}): {json_df.count()}")
    
    if json_df.count() == 0:
        return None
    
    # Afficher quelques exemples
    print(f"=== EXEMPLES DE DONN√âES JSON ({topic_name}) ===")
    json_df.select("json_data").show(3, truncate=False)
    
    return json_df

def write_transactions_to_minio(df, output_path):
    """√âcrire les donn√©es transactions vers MinIO en format Delta"""
    
    if df is None or df.count() == 0:
        print("Aucune donn√©e transactions √† √©crire dans MinIO")
        return
    
    print(f"√âcriture de {df.count()} enregistrements transactions vers MinIO...")
    
    try:
        # √âcriture en format Delta avec partitioning
        df.write \
            .format("delta") \
            .mode("append") \
            .partitionBy("year", "month", "day") \
            .option("mergeSchema", "true") \
            .save(output_path)
        
        print(f"Donn√©es transactions √©crites avec succ√®s dans: {output_path}")
        
        # Afficher quelques statistiques
        print("=== STATISTIQUES DES DONN√âES TRANSACTIONS √âCRITES ===")
        df.groupBy("payment_status").count().show()
        df.groupBy("category").count().show()
        
    except Exception as e:
        print(f"Erreur lors de l'√©criture Delta vers MinIO: {str(e)}")
        # Fallback vers Parquet si Delta √©choue
        print("Tentative d'√©criture en format Parquet...")
        try:
            df.write \
                .mode("append") \
                .partitionBy("year", "month", "day") \
                .parquet(output_path.replace("delta-data", "parquet-data"))
            print("Donn√©es √©crites en format Parquet avec succ√®s!")
        except Exception as e2:
            print(f"Erreur Parquet aussi: {str(e2)}")
            # Dernier recours: √©criture simple sans partitioning
            print("√âcriture simple sans partitioning...")
            df.write \
                .mode("append") \
                .parquet(output_path.replace("delta-data", "simple-parquet"))
            print("Donn√©es √©crites en format Parquet simple!")

def write_json_to_minio(df, topic_name, bucket_path):
    """√âcrire les donn√©es JSON vers MinIO avec gestion des conflits de fichiers"""
    
    if df is None or df.count() == 0:
        print(f"Aucune donn√©e {topic_name} √† √©crire dans MinIO")
        return
    
    print(f"√âcriture de {df.count()} enregistrements {topic_name} vers MinIO...")
    
    try:
        # Ajouter colonnes de partitioning bas√©es sur la date de traitement
        df_with_partitions = df.withColumn("year", year(col("processing_timestamp"))) \
                               .withColumn("month", month(col("processing_timestamp"))) \
                               .withColumn("day", dayofmonth(col("processing_timestamp"))) \
                               .withColumn("hour", hour(col("processing_timestamp"))) \
                               .withColumn("batch_id", lit(int(current_timestamp().cast("long").collect()[0][0] / 1000)))
        
        # √âcriture en format JSON avec partitioning par date et heure pour √©viter les conflits
        output_path = f"s3a://{bucket_path}"
        
        # Utiliser coalesce pour r√©duire le nombre de fichiers et √©viter les conflits
        df_coalesced = df_with_partitions.coalesce(1)
        
        df_coalesced.write \
            .mode("append") \
            .partitionBy("year", "month", "day", "hour") \
            .option("compression", "gzip") \
            .option("maxRecordsPerFile", 10000) \
            .json(output_path)
        
        print(f"Donn√©es {topic_name} √©crites avec succ√®s dans: {output_path}")
        
        # Afficher quelques statistiques
        print(f"=== STATISTIQUES DES DONN√âES {topic_name.upper()} √âCRITES ===")
        print(f"Nombre total d'enregistrements: {df.count()}")
        df.select("kafka_partition").distinct().show()
        
    except Exception as e:
        if "FileAlreadyExistsException" in str(e) or "destination file exists" in str(e):
            print(f"‚ö†Ô∏è Conflit de fichiers pour {topic_name}, utilisation d'un timestamp unique...")
            try:
                # Ajouter un timestamp unique pour √©viter les conflits
                import time
                timestamp_suffix = int(time.time() * 1000)  # timestamp en millisecondes
                
                df_with_unique_id = df.withColumn("batch_timestamp", lit(timestamp_suffix))
                
                # Chemin avec timestamp unique
                unique_path = f"s3a://{bucket_path}/batch_{timestamp_suffix}"
                
                df_with_unique_id.coalesce(1).write \
                    .mode("overwrite") \
                    .option("compression", "gzip") \
                    .json(unique_path)
                
                print(f"‚úÖ Donn√©es {topic_name} √©crites avec timestamp unique: {unique_path}")
                
            except Exception as e3:
                print(f"‚ùå Erreur m√™me avec timestamp unique: {str(e3)}")
                raise e3
        else:
            print(f"Erreur lors de l'√©criture JSON {topic_name} vers MinIO: {str(e)}")
            # Fallback: √©criture avec timestamp unique
            try:
                print(f"Tentative d'√©criture avec timestamp unique pour {topic_name}...")
                import time
                timestamp_suffix = int(time.time() * 1000)
                unique_path = f"s3a://{bucket_path}/batch_{timestamp_suffix}"
                
                df.coalesce(1).write \
                    .mode("overwrite") \
                    .option("compression", "gzip") \
                    .json(unique_path)
                    
                print(f"‚úÖ Donn√©es {topic_name} √©crites avec timestamp unique!")
            except Exception as e2:
                print(f"‚ùå Erreur lors de l'√©criture avec timestamp: {str(e2)}")
                raise e2

def validate_data(df, data_type="transactions"):
    """Valider les donn√©es avant √©criture"""
    print(f"\n=== VALIDATION DES DONN√âES {data_type.upper()} ===")
    
    if df is None or df.count() == 0:
        print("AUCUNE DONN√âE √Ä VALIDER!")
        return
    
    # Statistiques g√©n√©rales
    total_count = df.count()
    print(f"Nombre total d'enregistrements: {total_count}")
    
    if data_type == "transactions":
        # V√©rifier les valeurs nulles dans les colonnes importantes
        important_cols = ["order_id", "customer_id", "amount", "timestamp", "payment_status"]
        for col_name in important_cols:
            if col_name in df.columns:
                null_count = df.filter(col(col_name).isNull()).count()
                print(f"Valeurs nulles dans {col_name}: {null_count}")
        
        # V√©rifier la plage de dates
        if "timestamp" in df.columns:
            date_range = df.select(min("timestamp").alias("date_min"), max("timestamp").alias("date_max"))
            print("Plage de dates:")
            date_range.show()
        
        # V√©rifier les montants
        if "amount" in df.columns:
            amount_stats = df.select(
                min("amount").alias("amount_min"),
                max("amount").alias("amount_max"),
                avg("amount").alias("amount_avg")
            )
            print("Statistiques des montants:")
            amount_stats.show()
            
        # V√©rifier les doublons d'order_id
        if "order_id" in df.columns:
            unique_orders = df.select("order_id").distinct().count()
            print(f"Ordres uniques: {unique_orders}/{total_count}")
            if unique_orders < total_count:
                print(f"‚ö†Ô∏è {total_count - unique_orders} doublons d'order_id d√©tect√©s!")
    else:
        # Validation pour les donn√©es JSON
        print(f"Validation des donn√©es JSON pour {data_type}")
        if "json_data" in df.columns:
            null_json_count = df.filter(col("json_data").isNull()).count()
            print(f"Messages JSON nulls: {null_json_count}")
        
        if "kafka_partition" in df.columns:
            print("Distribution par partition Kafka:")
            df.groupBy("kafka_partition").count().show()
    
    print(f"=== FIN VALIDATION {data_type.upper()} ===\n")

def main():
    """Fonction principale avec parser CSV corrig√©"""
    print("=== D√âMARRAGE DU JOB KAFKA VERS MINIO (PARSER CSV CORRIG√â) ===")
    
    # Cr√©er la session Spark
    spark = create_spark_session()
    
    try:
        # S'assurer que le bucket kafka-offsets existe
        ensure_offset_bucket_exists(spark)
        
        # Configuration des topics avec le nouveau processor
        topics_config = {
            "transactions": {
                "processor": process_transactions_topic_fixed,  # NOUVEAU PROCESSOR
                "writer": write_transactions_to_minio,
                "output_path": "s3a://transactions/raw/delta-data"
            },
            "social_media": {
                "processor": lambda spark: process_json_topic(spark, "social_media", "social-media/raw"),
                "writer": lambda df, path: write_json_to_minio(df, "social_media", "social-media/raw"),
                "output_path": "social-media/raw"
            },
            "iot_sensors": {
                "processor": lambda spark: process_json_topic(spark, "iot_sensors", "iot-sensors/raw"),
                "writer": lambda df, path: write_json_to_minio(df, "iot_sensors", "iot-sensors/raw"),
                "output_path": "iot-sensors/raw"
            },
            "customer_images": {
                "processor": lambda spark: process_json_topic(spark, "customer_images", "customer-images/raw"),
                "writer": lambda df, path: write_json_to_minio(df, "customer_images", "customer-images/raw"),
                "output_path": "customer-images/raw"
            }
        }
        
        # Traiter chaque topic
        for topic_name, config in topics_config.items():
            print(f"\n{'='*70}")
            print(f"TRAITEMENT DU TOPIC: {topic_name.upper()}")
            print(f"{'='*70}")
            
            try:
                # Traiter les donn√©es du topic
                df = config["processor"](spark)
                
                if df is not None and df.count() > 0:
                    # Valider les donn√©es
                    validate_data(df, topic_name)
                    
                    # √âcrire vers MinIO
                    config["writer"](df, config["output_path"])
                    
                    # CRUCIAL: Sauvegarder les offsets APR√àS l'√©criture r√©ussie
                    save_last_offsets(spark, df, topic_name)
                    
                    print(f"‚úÖ Topic {topic_name} trait√© avec succ√®s! {df.count()} enregistrements trait√©s.")
                else:
                    print(f"‚ö†Ô∏è Aucune nouvelle donn√©e √† traiter pour {topic_name}")
                    
            except Exception as e:
                print(f"‚ùå Erreur lors du traitement de {topic_name}: {str(e)}")
                print(f"‚ö†Ô∏è Les offsets pour {topic_name} ne seront pas mis √† jour")
                import traceback
                traceback.print_exc()
                # Continuer avec les autres topics
                continue
        
        print(f"\n{'='*70}")
        print("=== TRAITEMENT TERMIN√â ===")
        print(f"{'='*70}")
        
    except Exception as e:
        print(f"ERREUR GLOBALE lors du traitement: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e
    finally:
        print("Fermeture de la session Spark...")
        spark.stop()

if __name__ == "__main__":
    main()

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta import DeltaTable
import json

def create_spark_session():
    """CrÃ©er une session Spark avec les configurations nÃ©cessaires pour Delta Lake"""
    spark = SparkSession.builder \
        .appName("JSONDiagnosticTool") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio-api.minio.svc.cluster.local:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark

def diagnose_json_files(spark):
    """Diagnostiquer les fichiers JSON pour comprendre le problÃ¨me"""
    print("=" * 80)
    print("ğŸ” DIAGNOSTIC DES FICHIERS JSON SOCIAL MEDIA")
    print("=" * 80)
    
    json_source_path = "s3a://social-media/raw"
    
    try:
        print(f"ğŸ“ Analyse du dossier: {json_source_path}")
        
        # 1. DÃ©couvrir tous les fichiers JSON
        print("\n1ï¸âƒ£ DÃ‰COUVERTE DES FICHIERS...")
        
        json_files = []
        try:
            # MÃ©thode rÃ©cursive pour trouver tous les fichiers
            temp_df = spark.read.option("recursiveFileLookup", "true").text(json_source_path)
            input_files = temp_df.inputFiles()
            json_files = [f for f in input_files if (f.endswith('.json') or f.endswith('.json.gz')) and not f.endswith('_SUCCESS')]
            print(f"   âœ… {len(json_files)} fichiers JSON trouvÃ©s")
        except Exception as e:
            print(f"   âŒ Erreur: {str(e)}")
            return
        
        if not json_files:
            print("   âš ï¸ Aucun fichier JSON trouvÃ©")
            return
        
        # 2. Analyser chaque fichier individuellement
        print(f"\n2ï¸âƒ£ ANALYSE DÃ‰TAILLÃ‰E DES FICHIERS...")
        
        total_records = 0
        
        for i, file_path in enumerate(json_files[:10]):  # Analyser maximum 10 fichiers pour le diagnostic
            print(f"\nğŸ“„ Fichier {i+1}: {file_path.split('/')[-1]}")
            
            try:
                # MÃ©thode 1: Lecture avec schÃ©ma automatique (plus flexible)
                print(f"   ğŸ”„ MÃ©thode 1: Lecture automatique...")
                
                if file_path.endswith('.json.gz'):
                    df_auto = spark.read \
                        .option("multiline", "true") \
                        .option("mode", "PERMISSIVE") \
                        .option("compression", "gzip") \
                        .json(file_path)
                else:
                    df_auto = spark.read \
                        .option("multiline", "true") \
                        .option("mode", "PERMISSIVE") \
                        .json(file_path)
                
                count_auto = df_auto.count()
                print(f"   ğŸ“Š Nombre de lignes (auto): {count_auto}")
                
                if count_auto > 0:
                    print(f"   ğŸ“‹ Colonnes dÃ©tectÃ©es: {df_auto.columns}")
                    print(f"   ğŸ” SchÃ©ma:")
                    df_auto.printSchema()
                    
                    # Afficher quelques exemples
                    print(f"   ğŸ“ Exemple de donnÃ©es:")
                    df_auto.show(2, truncate=True)
                    
                    total_records += count_auto
                else:
                    print(f"   âš ï¸ Fichier vide ou non lisible")
                
                # MÃ©thode 2: Lecture en tant que texte brut pour voir le contenu rÃ©el
                print(f"   ğŸ”„ MÃ©thode 2: Lecture en texte brut...")
                
                if file_path.endswith('.json.gz'):
                    text_df = spark.read \
                        .option("compression", "gzip") \
                        .text(file_path)
                else:
                    text_df = spark.read.text(file_path)
                
                text_lines = text_df.count()
                print(f"   ğŸ“Š Nombre de lignes de texte: {text_lines}")
                
                if text_lines > 0:
                    print(f"   ğŸ“ PremiÃ¨res lignes de texte:")
                    sample_lines = text_df.limit(3).collect()
                    for j, row in enumerate(sample_lines):
                        line_preview = row.value[:100] + "..." if len(row.value) > 100 else row.value
                        print(f"      Ligne {j+1}: {line_preview}")
                
            except Exception as file_error:
                print(f"   âŒ Erreur lors de l'analyse: {str(file_error)}")
        
        print(f"\nğŸ“Š RÃ‰SUMÃ‰:")
        print(f"   Total fichiers analysÃ©s: {min(len(json_files), 10)}")
        print(f"   Total fichiers disponibles: {len(json_files)}")
        print(f"   Total enregistrements trouvÃ©s: {total_records}")
        
        if len(json_files) > 10:
            print(f"   âš ï¸ {len(json_files) - 10} fichiers supplÃ©mentaires non analysÃ©s (limitation)")
        
        # 3. Test de lecture globale avec diffÃ©rentes approches
        print(f"\n3ï¸âƒ£ TEST DE LECTURE GLOBALE...")
        
        try:
            print("   ğŸ”„ Lecture globale avec schÃ©ma automatique...")
            global_df = spark.read \
                .option("recursiveFileLookup", "true") \
                .option("multiline", "true") \
                .option("mode", "PERMISSIVE") \
                .json(json_source_path)
            
            global_count = global_df.count()
            print(f"   ğŸ“Š Total enregistrements (lecture globale): {global_count}")
            
            if global_count > 0:
                print(f"   ğŸ“‹ Colonnes dÃ©tectÃ©es globalement: {global_df.columns}")
                print(f"   ğŸ“ Ã‰chantillon global:")
                global_df.show(5, truncate=True)
            
        except Exception as global_error:
            print(f"   âŒ Erreur lecture globale: {str(global_error)}")
        
        # 4. VÃ©rifier les fichiers traitÃ©s
        print(f"\n4ï¸âƒ£ VÃ‰RIFICATION DES MÃ‰TADONNÃ‰ES DE TRAITEMENT...")
        
        try:
            processed_files_path = "s3a://social-media/metadata/processed_json_files.json"
            processed_df = spark.read.json(processed_files_path)
            processed_count = processed_df.count()
            print(f"   ğŸ“Š Fichiers marquÃ©s comme traitÃ©s: {processed_count}")
            
            if processed_count > 0:
                print(f"   ğŸ“‹ Liste des fichiers traitÃ©s:")
                processed_df.select("file_path", "record_count").show(truncate=False)
        except Exception as metadata_error:
            print(f"   âš ï¸ Pas de mÃ©tadonnÃ©es de traitement trouvÃ©es: {str(metadata_error)}")
        
    except Exception as e:
        print(f"âŒ Erreur gÃ©nÃ©rale lors du diagnostic: {str(e)}")
        import traceback
        traceback.print_exc()

def test_delta_table_creation(spark):
    """Tester la crÃ©ation d'une table Delta avec donnÃ©es de test"""
    print(f"\n5ï¸âƒ£ TEST DE CRÃ‰ATION TABLE DELTA...")
    
    try:
        # CrÃ©er des donnÃ©es de test
        test_data = [
            ("key1", "2025-08-01T10:00:00Z", "user1", "Twitter", "Test message 1", 10, 2),
            ("key2", "2025-08-01T11:00:00Z", "user2", "Facebook", "Test message 2", 20, 5),
            ("key3", "2025-08-01T12:00:00Z", "user3", "Instagram", "Test message 3", 30, 8)
        ]
        
        schema = StructType([
            StructField("kafka_key", StringType(), True),
            StructField("timestamp", StringType(), True),
            StructField("user_id", StringType(), True),
            StructField("platform", StringType(), True),
            StructField("content", StringType(), True),
            StructField("likes", IntegerType(), True),
            StructField("shares", IntegerType(), True)
        ])
        
        test_df = spark.createDataFrame(test_data, schema)
        test_count = test_df.count()
        print(f"   ğŸ“Š DonnÃ©es de test crÃ©Ã©es: {test_count} lignes")
        
        # Tester l'Ã©criture Delta
        test_delta_path = "s3a://social-media/test-delta"
        
        test_df.write \
            .format("delta") \
            .mode("overwrite") \
            .save(test_delta_path)
        
        print(f"   âœ… Table Delta de test crÃ©Ã©e avec succÃ¨s")
        
        # VÃ©rifier la lecture
        delta_test_df = spark.read.format("delta").load(test_delta_path)
        delta_count = delta_test_df.count()
        print(f"   ğŸ“Š DonnÃ©es lues depuis Delta: {delta_count} lignes")
        
        if delta_count == test_count:
            print(f"   âœ… Delta Lake fonctionne correctement")
        else:
            print(f"   âš ï¸ ProblÃ¨me avec Delta Lake: {test_count} Ã©crites, {delta_count} lues")
        
    except Exception as delta_error:
        print(f"   âŒ Erreur test Delta: {str(delta_error)}")

def main():
    """Fonction principale de diagnostic"""
    print("ğŸš€ DÃ‰MARRAGE DU DIAGNOSTIC JSON SOCIAL MEDIA")
    
    spark = create_spark_session()
    
    try:
        # Diagnostic complet
        diagnose_json_files(spark)
        
        # Test Delta Lake
        test_delta_table_creation(spark)
        
        print(f"\nâœ… DIAGNOSTIC TERMINÃ‰")
        print(f"\nğŸ’¡ RECOMMANDATIONS:")
        print(f"   1. VÃ©rifiez si le nombre total d'enregistrements correspond Ã  vos attentes")
        print(f"   2. Si les fichiers contiennent plus de donnÃ©es, le problÃ¨me peut Ãªtre:")
        print(f"      - SchÃ©ma trop rigide dans le script original")
        print(f"      - Filtres trop restrictifs")
        print(f"      - Mauvaise gestion des fichiers dÃ©jÃ  traitÃ©s")
        print(f"   3. Utilisez ce diagnostic pour identifier la vraie cause")
        
    except Exception as e:
        print(f"âŒ Erreur lors du diagnostic: {str(e)}")
        import traceback
        traceback.print_exc()
    
    finally:
        print("ğŸ”š Fermeture de la session Spark...")
        spark.stop()

if __name__ == "__main__":
    main()

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta import DeltaTable
import os
from datetime import datetime

def create_spark_session():
    """CrÃ©er une session Spark avec les configurations nÃ©cessaires pour Delta Lake"""
    spark = SparkSession.builder \
        .appName("CSVToDeltaLakeProcessor") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio-api.minio.svc.cluster.local:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.kubernetes.file.upload.path", "/tmp") \
        .config("spark.kubernetes.executor.deleteOnTermination", "true") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark

def get_processed_files_path():
    """Obtenir le chemin du fichier qui stocke la liste des fichiers dÃ©jÃ  traitÃ©s"""
    return "s3a://transactions/metadata/processed_files.json"

def ensure_metadata_bucket_exists(spark):
    """CrÃ©er le dossier metadata s'il n'existe pas"""
    try:
        test_df = spark.createDataFrame([("test",)], ["value"])
        test_path = "s3a://transactions/metadata/_test_metadata_exists"
        test_df.write.mode("overwrite").json(test_path)
        test_df.limit(0).write.mode("overwrite").json(test_path)
        print("âœ… Dossier metadata existe et est accessible")
    except Exception as e:
        print(f"ğŸ“ CrÃ©ation du dossier metadata: {str(e)}")

def read_processed_files_list(spark):
    """Lire la liste des fichiers dÃ©jÃ  traitÃ©s"""
    processed_files_path = get_processed_files_path()
    
    try:
        schema = StructType([
            StructField("file_path", StringType(), True),
            StructField("processed_at", TimestampType(), True),
            StructField("file_size", LongType(), True)
        ])
        
        processed_files_df = spark.read.schema(schema).json(processed_files_path)
        
        if processed_files_df.count() > 0:
            processed_files = [row.file_path for row in processed_files_df.collect()]
            print(f"ğŸ“„ {len(processed_files)} fichiers dÃ©jÃ  traitÃ©s trouvÃ©s")
            print("Fichiers traitÃ©s:")
            for f in processed_files:
                print(f"  - {f}")
            return set(processed_files)
        else:
            print("ğŸ“„ Aucun fichier traitÃ© prÃ©cÃ©demment (premiÃ¨re exÃ©cution)")
            return set()
            
    except Exception as e:
        if "Path does not exist" in str(e) or "NoSuchKey" in str(e):
            print("ğŸ“„ Pas de fichier de mÃ©tadonnÃ©es (premiÃ¨re exÃ©cution)")
        else:
            print(f"âš ï¸ Erreur lors de la lecture des mÃ©tadonnÃ©es: {str(e)}")
        return set()

def save_processed_files_list(spark, new_files):
    """Sauvegarder la liste mise Ã  jour des fichiers traitÃ©s"""
    if not new_files:
        print("â„¹ï¸ Aucun nouveau fichier Ã  ajouter aux mÃ©tadonnÃ©es")
        return
    
    try:
        print(f"ğŸ’¾ Sauvegarde de {len(new_files)} nouveaux fichiers dans les mÃ©tadonnÃ©es...")
        
        # Lire les fichiers dÃ©jÃ  traitÃ©s
        existing_files = read_processed_files_list(spark)
        print(f"ğŸ“‹ Fichiers existants: {len(existing_files)}")
        
        # Combiner avec les nouveaux fichiers
        all_files = existing_files.union(new_files)
        print(f"ğŸ“‹ Total des fichiers aprÃ¨s ajout: {len(all_files)}")
        
        # CrÃ©er le DataFrame avec les mÃ©tadonnÃ©es
        files_data = []
        for file_path in all_files:
            files_data.append({
                "file_path": file_path,
                "processed_at": datetime.now(),
                "file_size": 0  # On pourrait calculer la taille rÃ©elle si nÃ©cessaire
            })
        
        files_df = spark.createDataFrame(files_data)
        
        processed_files_path = get_processed_files_path()
        
        # CORRECTION: Utiliser un seul fichier de sortie et Ã©craser complÃ¨tement
        files_df.coalesce(1).write \
               .mode("overwrite") \
               .json(processed_files_path)
        
        print(f"âœ… Liste des fichiers traitÃ©s mise Ã  jour: {len(all_files)} fichiers au total")
        
        # VÃ©rification immÃ©diate pour confirmer la sauvegarde
        verification_df = spark.read.json(processed_files_path)
        verification_count = verification_df.count()
        print(f"ğŸ” VÃ©rification: {verification_count} fichiers sauvegardÃ©s dans les mÃ©tadonnÃ©es")
        
        if verification_count != len(all_files):
            print(f"âš ï¸ ATTENTION: Discordance entre attendu ({len(all_files)}) et sauvegardÃ© ({verification_count})")
        else:
            print("âœ… Sauvegarde des mÃ©tadonnÃ©es confirmÃ©e")
        
    except Exception as e:
        print(f"âŒ Erreur lors de la sauvegarde des mÃ©tadonnÃ©es: {str(e)}")
        import traceback
        traceback.print_exc()

def discover_csv_files(spark):
    """DÃ©couvrir tous les fichiers CSV dans le dossier source partitionnÃ©"""
    csv_source_path = "s3a://transactions/raw/csv-data"
    
    try:
        # Lister tous les fichiers CSV rÃ©cursivement avec la structure partitionnÃ©e
        csv_files = []
        
        print(f"ğŸ” Recherche des fichiers CSV dans {csv_source_path} (structure partitionnÃ©e)...")
        
        try:
            # Lire avec la structure de partitionnement year=*/month=*/day=*
            # Utilise le pattern pour les partitions Hive
            partition_pattern = f"{csv_source_path}/year=*/month=*/day=*"
            print(f"ğŸ“ Pattern de recherche: {partition_pattern}")
            
            temp_df = spark.read.option("header", "true").csv(partition_pattern)
            input_files = temp_df.inputFiles()
            csv_files = [f for f in input_files if f.endswith('.csv') and not f.endswith('_SUCCESS')]
            
            print(f"âœ… MÃ©thode partitionnÃ©e rÃ©ussie: {len(csv_files)} fichiers trouvÃ©s")
            
        except Exception as partition_error:
            print(f"âš ï¸ Erreur avec pattern partitionnÃ©: {str(partition_error)}")
            print("ğŸ”„ Tentative avec mÃ©thode rÃ©cursive...")
            
            try:
                # Fallback: essayer de lire rÃ©cursivement tous les sous-dossiers
                temp_df = spark.read.option("header", "true").option("recursiveFileLookup", "true").csv(csv_source_path)
                input_files = temp_df.inputFiles()
                csv_files = [f for f in input_files if f.endswith('.csv') and not f.endswith('_SUCCESS')]
                
                print(f"âœ… MÃ©thode rÃ©cursive rÃ©ussie: {len(csv_files)} fichiers trouvÃ©s")
                
            except Exception as recursive_error:
                print(f"âš ï¸ Erreur avec mÃ©thode rÃ©cursive: {str(recursive_error)}")
                print("ğŸ”„ Tentative avec lecture directe...")
                
                try:
                    # DerniÃ¨re tentative: lecture directe
                    temp_df = spark.read.option("header", "true").csv(csv_source_path)
                    input_files = temp_df.inputFiles()
                    csv_files = [f for f in input_files if f.endswith('.csv') and not f.endswith('_SUCCESS')]
                    
                    print(f"âœ… MÃ©thode directe rÃ©ussie: {len(csv_files)} fichiers trouvÃ©s")
                    
                except Exception as direct_error:
                    print(f"âŒ Toutes les mÃ©thodes ont Ã©chouÃ©: {str(direct_error)}")
                    return []
        
        # Filtrer et afficher les fichiers trouvÃ©s
        if csv_files:
            print(f"ğŸ“‚ {len(csv_files)} fichiers CSV dÃ©couverts:")
            
            # Grouper par partition pour un affichage plus clair
            partition_groups = {}
            for file in csv_files:
                # Extraire la partition du chemin
                if "/year=" in file and "/month=" in file and "/day=" in file:
                    parts = file.split("/")
                    year_part = [p for p in parts if p.startswith("year=")]
                    month_part = [p for p in parts if p.startswith("month=")]
                    day_part = [p for p in parts if p.startswith("day=")]
                    
                    if year_part and month_part and day_part:
                        partition_key = f"{year_part[0]}/{month_part[0]}/{day_part[0]}"
                        if partition_key not in partition_groups:
                            partition_groups[partition_key] = []
                        partition_groups[partition_key].append(file)
                else:
                    # Fichier sans partition claire
                    if "other" not in partition_groups:
                        partition_groups["other"] = []
                    partition_groups["other"].append(file)
            
            # Afficher par partition
            for partition, files in sorted(partition_groups.items()):
                print(f"  ğŸ“ {partition}: {len(files)} fichier(s)")
                for file in files[:2]:  # Afficher les 2 premiers de chaque partition
                    print(f"    - {file}")
                if len(files) > 2:
                    print(f"    ... et {len(files) - 2} autres fichiers")
        else:
            print("âš ï¸ Aucun fichier CSV trouvÃ©")
        
        return csv_files
        
    except Exception as e:
        print(f"âŒ Erreur lors de la dÃ©couverte des fichiers CSV: {str(e)}")
        import traceback
        traceback.print_exc()
        return []

def read_new_csv_files(spark, csv_files, processed_files):
    """Lire uniquement les nouveaux fichiers CSV non traitÃ©s"""
    new_files = [f for f in csv_files if f not in processed_files]
    
    if not new_files:
        print("âœ… Tous les fichiers CSV ont dÃ©jÃ  Ã©tÃ© traitÃ©s")
        return None, set()
    
    print(f"ğŸ“Š {len(new_files)} nouveaux fichiers CSV Ã  traiter:")
    for file in new_files[:3]:
        print(f"  - {file}")
    if len(new_files) > 3:
        print(f"  ... et {len(new_files) - 3} autres fichiers")
    
    try:
        # DÃ©finir le schÃ©ma explicite pour les transactions
        transaction_schema = StructType([
            StructField("order_id", StringType(), True),
            StructField("customer_id", StringType(), True),
            StructField("customer_first_name", StringType(), True),
            StructField("customer_last_name", StringType(), True),
            StructField("product_id", StringType(), True),
            StructField("product_name", StringType(), True),
            StructField("category", StringType(), True),
            StructField("amount", StringType(), True),
            StructField("payment_method", StringType(), True),
            StructField("payment_status", StringType(), True),
            StructField("discount_code", StringType(), True),
            StructField("shipping_address", StringType(), True),
            StructField("timestamp_raw", StringType(), True),
            StructField("is_returned", StringType(), True),
            StructField("kafka_timestamp", TimestampType(), True),
            StructField("kafka_partition", LongType(), True),
            StructField("kafka_offset", LongType(), True),
            StructField("processing_date", DateType(), True),
            StructField("processing_timestamp", TimestampType(), True),
            StructField("year", IntegerType(), True),
            StructField("month", IntegerType(), True),
            StructField("day", IntegerType(), True)
        ])
        
        # Option 1: Essayer de lire tous les nouveaux fichiers en une seule fois
        try:
            print("ğŸ”„ Tentative de lecture groupÃ©e des nouveaux fichiers...")
            combined_df = spark.read \
                .option("header", "true") \
                .option("inferSchema", "false") \
                .schema(transaction_schema) \
                .csv(new_files)
            
            # Ajouter une colonne pour identifier les fichiers sources
            combined_df = combined_df.withColumn("source_file", input_file_name())
            
            total_records = combined_df.count()
            print(f"âœ… Lecture groupÃ©e rÃ©ussie: {total_records} enregistrements")
            
        except Exception as batch_error:
            print(f"âš ï¸ Lecture groupÃ©e Ã©chouÃ©e: {str(batch_error)}")
            print("ğŸ”„ Passage Ã  la lecture fichier par fichier...")
            
            # Option 2: Lire fichier par fichier (fallback)
            combined_df = None
            
            for file_path in new_files:
                try:
                    print(f"  ğŸ“„ Lecture de: {file_path}")
                    
                    file_df = spark.read \
                        .option("header", "true") \
                        .option("inferSchema", "false") \
                        .schema(transaction_schema) \
                        .csv(file_path)
                    
                    file_count = file_df.count()
                    print(f"    âœ… {file_count} enregistrements")
                    
                    if file_count > 0:
                        # Ajouter le nom du fichier source
                        file_df = file_df.withColumn("source_file", lit(file_path))
                        
                        if combined_df is None:
                            combined_df = file_df
                        else:
                            combined_df = combined_df.union(file_df)
                    else:
                        print(f"    âš ï¸ Fichier vide ignorÃ©: {file_path}")
                        
                except Exception as file_error:
                    print(f"    âŒ Erreur lors de la lecture de {file_path}: {str(file_error)}")
                    continue
        
        if combined_df is not None:
            total_records = combined_df.count()
            print(f"âœ… Total des nouveaux enregistrements: {total_records}")
            
            # Nettoyer et valider les donnÃ©es
            cleaned_df = combined_df.filter(
                col("order_id").isNotNull() & 
                (col("order_id") != "order_id") &
                col("customer_id").isNotNull() & 
                (col("customer_id") != "customer_id") &
                col("amount").isNotNull() &
                (col("amount") != "amount")
            )
            
            # CORRECTION CRITIQUE: GÃ©rer les colonnes de partitionnement NULL
            print("ğŸ”§ Correction des colonnes de partitionnement...")
            
            # VÃ©rifier l'Ã©tat actuel des colonnes de partitionnement
            print("Ã‰tat des colonnes de partitionnement avant correction:")
            cleaned_df.select("processing_date", "year", "month", "day").show(5)
            
            # Option 1: Si processing_date existe et est valide
            processing_date_not_null = cleaned_df.filter(col("processing_date").isNotNull()).count()
            total_cleaned = cleaned_df.count()
            
            print(f"processing_date non-null: {processing_date_not_null}/{total_cleaned}")
            
            if processing_date_not_null > 0:
                # Utiliser processing_date pour gÃ©nÃ©rer les partitions
                final_df = cleaned_df.withColumn("year", year(col("processing_date"))) \
                                   .withColumn("month", month(col("processing_date"))) \
                                   .withColumn("day", dayofmonth(col("processing_date")))
                print("âœ… Utilisation de processing_date pour les partitions")
            else:
                # Option 2: Si processing_timestamp existe
                processing_ts_not_null = cleaned_df.filter(col("processing_timestamp").isNotNull()).count()
                print(f"processing_timestamp non-null: {processing_ts_not_null}/{total_cleaned}")
                
                if processing_ts_not_null > 0:
                    final_df = cleaned_df.withColumn("processing_date", to_date(col("processing_timestamp"))) \
                                       .withColumn("year", year(col("processing_timestamp"))) \
                                       .withColumn("month", month(col("processing_timestamp"))) \
                                       .withColumn("day", dayofmonth(col("processing_timestamp")))
                    print("âœ… Utilisation de processing_timestamp pour les partitions")
                else:
                    # Option 3: Utiliser la date actuelle comme fallback
                    print("âš ï¸ Aucune date valide trouvÃ©e, utilisation de la date actuelle")
                    current_date_val = current_date()
                    final_df = cleaned_df.withColumn("processing_date", current_date_val) \
                                       .withColumn("year", year(current_date_val)) \
                                       .withColumn("month", month(current_date_val)) \
                                       .withColumn("day", dayofmonth(current_date_val))
            
            # S'assurer que les colonnes de partitionnement ne sont pas NULL
            final_df = final_df.filter(
                col("year").isNotNull() & 
                col("month").isNotNull() & 
                col("day").isNotNull()
            )
            
            # Convertir les types de donnÃ©es et ajouter les mÃ©tadonnÃ©es
            final_df = final_df.withColumn("amount_numeric", col("amount").cast(DoubleType())) \
                             .withColumn("is_returned_bool", col("is_returned").cast(BooleanType())) \
                             .withColumn("delta_load_timestamp", current_timestamp())
            
            # VÃ©rifier le rÃ©sultat final
            final_count = final_df.count()
            print(f"âœ… Enregistrements valides aprÃ¨s nettoyage et correction: {final_count}")
            
            if final_count > 0:
                print("Ã‰tat final des colonnes de partitionnement:")
                final_df.select("processing_date", "year", "month", "day").show(5)
                
                # VÃ©rifier qu'il n'y a plus de NULL dans les colonnes de partitionnement
                null_partitions = final_df.filter(
                    col("year").isNull() | col("month").isNull() | col("day").isNull()
                ).count()
                
                if null_partitions > 0:
                    print(f"âš ï¸ ATTENTION: {null_partitions} enregistrements ont encore des partitions NULL!")
                    final_df = final_df.filter(
                        col("year").isNotNull() & 
                        col("month").isNotNull() & 
                        col("day").isNotNull()
                    )
                    print(f"Enregistrements aprÃ¨s filtrage des NULL: {final_df.count()}")
                else:
                    print("âœ… Toutes les colonnes de partitionnement sont valides")
            
            return final_df, set(new_files)
        else:
            print("âš ï¸ Aucun enregistrement valide trouvÃ© dans les nouveaux fichiers")
            return None, set(new_files)
            
    except Exception as e:
        print(f"âŒ Erreur lors de la lecture des fichiers CSV: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, set()

def create_or_update_delta_table(spark, df, delta_path):
    """CrÃ©er ou mettre Ã  jour la table Delta Lake avec partitionnement"""
    if df is None:
        print("âš ï¸ Aucune donnÃ©e Ã  Ã©crire dans Delta Lake")
        return
    
    print(f"ğŸ”„ Traitement de {df.count()} enregistrements pour Delta Lake...")
    
    try:
        # VALIDATION CRITIQUE: S'assurer que les colonnes de partitionnement sont valides
        print("ğŸ” Validation des colonnes de partitionnement...")
        
        # VÃ©rifier que les colonnes de partitionnement existent et ne sont pas NULL
        partition_columns = ["year", "month", "day"]
        for col_name in partition_columns:
            if col_name not in df.columns:
                print(f"âŒ Colonne de partitionnement manquante: {col_name}")
                return
            
            null_count = df.filter(col(col_name).isNull()).count()
            if null_count > 0:
                print(f"âŒ {null_count} valeurs NULL trouvÃ©es dans la colonne de partitionnement {col_name}")
                print("ğŸ› ï¸ Tentative de correction...")
                
                # Essayer de corriger en utilisant la date actuelle
                if col_name == "year":
                    df = df.withColumn(col_name, when(col(col_name).isNull(), year(current_date())).otherwise(col(col_name)))
                elif col_name == "month":
                    df = df.withColumn(col_name, when(col(col_name).isNull(), month(current_date())).otherwise(col(col_name)))
                elif col_name == "day":
                    df = df.withColumn(col_name, when(col(col_name).isNull(), dayofmonth(current_date())).otherwise(col(col_name)))
                
                # Re-vÃ©rifier
                remaining_null = df.filter(col(col_name).isNull()).count()
                if remaining_null > 0:
                    print(f"âŒ Impossible de corriger les valeurs NULL dans {col_name}")
                    return
                else:
                    print(f"âœ… Valeurs NULL corrigÃ©es dans {col_name}")
        
        # Afficher un Ã©chantillon des valeurs de partitionnement
        print("ğŸ“‹ Ã‰chantillon des colonnes de partitionnement:")
        df.select("year", "month", "day").distinct().orderBy("year", "month", "day").show()
        
        # S'assurer que les valeurs de partitionnement sont dans des plages valides
        year_stats = df.select(min("year").alias("min_year"), max("year").alias("max_year")).collect()[0]
        month_stats = df.select(min("month").alias("min_month"), max("month").alias("max_month")).collect()[0]
        day_stats = df.select(min("day").alias("min_day"), max("day").alias("max_day")).collect()[0]
        
        print(f"ğŸ“Š Plages de partitionnement:")
        print(f"  AnnÃ©es: {year_stats.min_year} - {year_stats.max_year}")
        print(f"  Mois: {month_stats.min_month} - {month_stats.max_month}")
        print(f"  Jours: {day_stats.min_day} - {day_stats.max_day}")
        
        # Validation des plages
        if year_stats.min_year < 2020 or year_stats.max_year > 2030:
            print("âš ï¸ AnnÃ©es suspectes dÃ©tectÃ©es")
        if month_stats.min_month < 1 or month_stats.max_month > 12:
            print("âŒ Mois invalides dÃ©tectÃ©s")
            df = df.filter((col("month") >= 1) & (col("month") <= 12))
        if day_stats.min_day < 1 or day_stats.max_day > 31:
            print("âŒ Jours invalides dÃ©tectÃ©s")
            df = df.filter((col("day") >= 1) & (col("day") <= 31))
        
        final_count_after_validation = df.count()
        print(f"ğŸ“Š Enregistrements aprÃ¨s validation: {final_count_after_validation}")
        
        if final_count_after_validation == 0:
            print("âŒ Aucun enregistrement valide aprÃ¨s validation des partitions")
            return
        
        # CORRECTION: Invalider le cache Spark avant de compter
        print("ğŸ”„ Invalidation du cache Spark...")
        spark.sql("CLEAR CACHE")
        
        # VÃ©rifier si la table Delta existe dÃ©jÃ 
        if DeltaTable.isDeltaTable(spark, delta_path):
            print("ğŸ“Š Table Delta existante trouvÃ©e - Mode APPEND avec partitionnement")
            
            # CORRECTION: RecrÃ©er la rÃ©fÃ©rence Ã  la table Delta pour Ã©viter les erreurs de cache
            try:
                # RafraÃ®chir la table Delta
                spark.sql(f"REFRESH TABLE delta.`{delta_path}`")
            except Exception as refresh_error:
                print(f"âš ï¸ Impossible de rafraÃ®chir la table: {str(refresh_error)}")
            
            # Charger la table Delta existante avec une nouvelle rÃ©fÃ©rence
            delta_table = DeltaTable.forPath(spark, delta_path)
            
            # Afficher les statistiques actuelles avec gestion d'erreur
            try:
                current_count = delta_table.toDF().count()
                print(f"ğŸ“ˆ Enregistrements actuels dans Delta: {current_count}")
            except Exception as count_error:
                print(f"âš ï¸ Impossible de compter les enregistrements existants: {str(count_error)}")
                print("ğŸ”„ Tentative de lecture directe...")
                try:
                    current_df = spark.read.format("delta").load(delta_path)
                    current_count = current_df.count()
                    print(f"ğŸ“ˆ Enregistrements actuels dans Delta (lecture directe): {current_count}")
                except Exception as direct_count_error:
                    print(f"âŒ Impossible de lire la table existante: {str(direct_count_error)}")
                    current_count = 0
            
            # Ajouter les nouvelles donnÃ©es avec partitionnement (APPEND)
            print("ğŸ’¾ Ã‰criture des nouvelles donnÃ©es en mode APPEND...")
            df.write \
              .format("delta") \
              .mode("append") \
              .partitionBy("year", "month", "day") \
              .option("mergeSchema", "true") \
              .save(delta_path)
            
            # Afficher les nouvelles statistiques
            try:
                updated_delta_table = DeltaTable.forPath(spark, delta_path)
                new_count = updated_delta_table.toDF().count()
                print(f"ğŸ“ˆ Enregistrements aprÃ¨s ajout: {new_count} (+{new_count - current_count})")
            except Exception as new_count_error:
                print(f"âš ï¸ Impossible de compter aprÃ¨s ajout: {str(new_count_error)}")
            
        else:
            print("ğŸ†• CrÃ©ation d'une nouvelle table Delta Lake avec partitionnement")
            
            # CrÃ©er la nouvelle table Delta avec partitionnement
            df.write \
              .format("delta") \
              .mode("overwrite") \
              .partitionBy("year", "month", "day") \
              .option("delta.autoOptimize.optimizeWrite", "true") \
              .option("delta.autoOptimize.autoCompact", "true") \
              .save(delta_path)
            
            print(f"âœ… Table Delta crÃ©Ã©e avec {df.count()} enregistrements")
            print("ğŸ“ Partitionnement: year/month/day")
        
        # Afficher des statistiques de la table mise Ã  jour
        print("\n=== STATISTIQUES DE LA TABLE DELTA PARTITIONNÃ‰E ===")
        try:
            delta_table = DeltaTable.forPath(spark, delta_path)
            final_df = delta_table.toDF()
            
            print(f"ğŸ“Š Total des enregistrements: {final_df.count()}")
            
            # Afficher la structure des partitions
            print("\nğŸ“ Structure des partitions:")
            partition_info = final_df.groupBy("year", "month", "day").count() \
                                    .orderBy("year", "month", "day")
            partition_info.show()
            
            # VÃ©rifier qu'il n'y a plus de partitions NULL
            null_partitions = final_df.filter(
                col("year").isNull() | col("month").isNull() | col("day").isNull()
            ).count()
            
            if null_partitions > 0:
                print(f"âŒ PROBLÃˆME CRITIQUE: {null_partitions} enregistrements avec partitions NULL dÃ©tectÃ©s!")
            else:
                print("âœ… Toutes les partitions sont valides")
            
            print("\nğŸ“Š RÃ©partition par statut de paiement:")
            final_df.groupBy("payment_status").count().orderBy(desc("count")).show()
            
            print("\nğŸ“Š RÃ©partition par catÃ©gorie (top 10):")
            final_df.groupBy("category").count().orderBy(desc("count")).show(10)
            
        except Exception as stats_error:
            print(f"âš ï¸ Impossible d'afficher les statistiques: {str(stats_error)}")
        
        # Optimiser la table Delta
        print("ğŸ”§ Optimisation de la table Delta partitionnÃ©e...")
        try:
            spark.sql(f"OPTIMIZE delta.`{delta_path}`")
            print("âœ… Table Delta optimisÃ©e")
        except Exception as opt_error:
            print(f"âš ï¸ Optimisation Ã©chouÃ©e: {str(opt_error)}")
        
        # Afficher des informations sur le partitionnement
        print(f"\nğŸ“‹ INFORMATIONS DE PARTITIONNEMENT:")
        print(f"ğŸ“ Chemin de la table: {delta_path}")
        print("ğŸ“ Structure attendue: /year=YYYY/month=MM/day=DD/")
        
        # Test de lecture d'une partition spÃ©cifique
        try:
            sample_partition = df.select("year", "month", "day").first()
            if sample_partition:
                test_partition_df = spark.read.format("delta").load(delta_path) \
                    .filter((col("year") == sample_partition.year) & 
                           (col("month") == sample_partition.month) & 
                           (col("day") == sample_partition.day))
                partition_count = test_partition_df.count()
                print(f"ğŸ§ª Test de lecture partition year={sample_partition.year}/month={sample_partition.month}/day={sample_partition.day}: {partition_count} enregistrements")
        except Exception as test_error:
            print(f"âš ï¸ Test de partition Ã©chouÃ©: {str(test_error)}")
        
    except Exception as e:
        print(f"âŒ Erreur lors de la crÃ©ation/mise Ã  jour de la table Delta: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e

def main():
    """Fonction principale pour convertir les CSV en Delta Lake"""
    print("=== DÃ‰MARRAGE DE LA CONVERSION CSV VERS DELTA LAKE ===")
    
    spark = create_spark_session()
    
    try:
        # S'assurer que le dossier metadata existe
        ensure_metadata_bucket_exists(spark)
        
        # DÃ©couvrir tous les fichiers CSV
        csv_files = discover_csv_files(spark)
        
        if not csv_files:
            print("âš ï¸ Aucun fichier CSV trouvÃ© Ã  traiter")
            return
        
        # Lire la liste des fichiers dÃ©jÃ  traitÃ©s
        processed_files = read_processed_files_list(spark)
        
        # Lire uniquement les nouveaux fichiers CSV
        new_data_df, new_files_processed = read_new_csv_files(spark, csv_files, processed_files)
        
        if new_data_df is not None and new_data_df.count() > 0:
            # Chemin de la table Delta Lake
            delta_table_path = "s3a://transactions/bronze"
            
            # CrÃ©er ou mettre Ã  jour la table Delta
            create_or_update_delta_table(spark, new_data_df, delta_table_path)
            
            # CORRECTION CRITIQUE: Sauvegarder les mÃ©tadonnÃ©es SEULEMENT aprÃ¨s succÃ¨s
            print("ğŸ’¾ Sauvegarde des fichiers traitÃ©s dans les mÃ©tadonnÃ©es...")
            save_processed_files_list(spark, new_files_processed)
            
            print(f"âœ… Conversion terminÃ©e avec succÃ¨s! {new_data_df.count()} nouveaux enregistrements ajoutÃ©s Ã  Delta Lake.")
            print(f"ğŸ“ Table Delta Lake disponible Ã : {delta_table_path}")
            
            # VÃ©rification finale des mÃ©tadonnÃ©es
            print("\n=== VÃ‰RIFICATION FINALE DES MÃ‰TADONNÃ‰ES ===")
            final_processed_files = read_processed_files_list(spark)
            print(f"ğŸ“‹ Nombre total de fichiers marquÃ©s comme traitÃ©s: {len(final_processed_files)}")
            
        else:
            print("â„¹ï¸ Aucune nouvelle donnÃ©e Ã  traiter")
            # MÃªme si pas de nouvelles donnÃ©es, marquer les fichiers comme traitÃ©s si ils Ã©taient dans la liste
            if new_files_processed:
                print("ğŸ’¾ Marquage des fichiers vides comme traitÃ©s...")
                save_processed_files_list(spark, new_files_processed)
            
    except Exception as e:
        print(f"âŒ Erreur lors de la conversion: {str(e)}")
        import traceback
        traceback.print_exc()
        # CORRECTION: Ne pas sauvegarder les mÃ©tadonnÃ©es en cas d'erreur
        print("âš ï¸ Les fichiers ne seront pas marquÃ©s comme traitÃ©s Ã  cause de l'erreur")
        raise e
    finally:
        print("ğŸ”š Fermeture de la session Spark...")
        spark.stop()

if __name__ == "__main__":
    main()

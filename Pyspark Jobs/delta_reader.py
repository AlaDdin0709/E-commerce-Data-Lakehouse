from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from delta import DeltaTable
import sys

def create_spark_session():
    """CrÃ©er une session Spark avec les configurations nÃ©cessaires"""
    spark = SparkSession.builder \
        .appName("DeltaTableRepair") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio-api.minio.svc.cluster.local:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark

def count_total_records(spark, table_path):
    """Compter le nombre total d'enregistrements dans la table Delta"""
    print("\nğŸ” COMPTAGE DES ENREGISTREMENTS TOTAUX")
    
    try:
        # MÃ©thode 1: Via SQL (la plus rapide)
        try:
            count_df = spark.sql(f"SELECT COUNT(*) as total_count FROM delta.`{table_path}`")
            total_count = count_df.collect()[0]["total_count"]
            print(f"âœ… Nombre total d'enregistrements (via SQL): {total_count:,}")
            return total_count
        except Exception as sql_error:
            print(f"âš ï¸ Impossible de compter via SQL: {str(sql_error)}")
            
        # MÃ©thode 2: Via DataFrame (plus lent mais plus robuste)
        try:
            total_count = spark.read.format("delta").load(table_path).count()
            print(f"âœ… Nombre total d'enregistrements (via DataFrame): {total_count:,}")
            return total_count
        except Exception as df_error:
            print(f"âŒ Impossible de compter via DataFrame: {str(df_error)}")
            return None
            
    except Exception as e:
        print(f"âŒ Erreur gÃ©nÃ©rale lors du comptage: {str(e)}")
        return None

def diagnose_delta_table_issues(spark, table_path):
    """Diagnostiquer les problÃ¨mes de la table Delta"""
    print(f"=== DIAGNOSTIC DE LA TABLE DELTA: {table_path} ===")
    
    try:
        # 1. VÃ©rifier que la table Delta existe
        if not DeltaTable.isDeltaTable(spark, table_path):
            print(f"âŒ ERREUR: Pas une table Delta valide au chemin {table_path}")
            return False
        
        print("âœ… Table Delta dÃ©tectÃ©e")
        
        # 2. Essayer de charger la table Delta
        try:
            delta_table = DeltaTable.forPath(spark, table_path)
            print("âœ… Table Delta chargÃ©e avec succÃ¨s")
        except Exception as load_error:
            print(f"âŒ Erreur lors du chargement de la table Delta: {str(load_error)}")
            return False
        
        # 3. VÃ©rifier l'historique Delta
        print("\nğŸ” VÃ‰RIFICATION DE L'HISTORIQUE DELTA:")
        try:
            history_df = delta_table.history()
            history_count = history_df.count()
            print(f"ğŸ“œ Nombre d'opÃ©rations dans l'historique: {history_count}")
            
            if history_count > 0:
                print("ğŸ“‹ DerniÃ¨res opÃ©rations:")
                history_df.select("version", "timestamp", "operation", "operationParameters") \
                         .orderBy(desc("version")) \
                         .show(5, truncate=False)
                
                # VÃ©rifier la version actuelle
                current_version = history_df.first().version
                print(f"ğŸ“ˆ Version actuelle: {current_version}")
            
        except Exception as history_error:
            print(f"âš ï¸ Erreur lors de la lecture de l'historique: {str(history_error)}")
        
        # 4. Essayer de lire les mÃ©tadonnÃ©es sans lire les donnÃ©es
        print("\nğŸ” VÃ‰RIFICATION DES MÃ‰TADONNÃ‰ES:")
        try:
            df_schema_only = spark.read.format("delta").load(table_path).limit(0)
            print(f"âœ… SchÃ©ma accessible: {len(df_schema_only.columns)} colonnes")
            print(f"ğŸ“‹ Colonnes: {df_schema_only.columns}")
        except Exception as schema_error:
            print(f"âŒ Erreur lors de la lecture du schÃ©ma: {str(schema_error)}")
        
        # 5. Compter le nombre total d'enregistrements
        total_records = count_total_records(spark, table_path)
        if total_records is not None:
            print(f"ğŸ“Š Total des enregistrements: {total_records:,}")
        
        # 6. Essayer de lire un petit Ã©chantillon
        print("\nğŸ” TEST DE LECTURE D'Ã‰CHANTILLON:")
        try:
            sample_df = spark.read.format("delta").load(table_path).limit(1)
            sample_count = sample_df.count()
            print(f"âœ… Ã‰chantillon lu avec succÃ¨s: {sample_count} enregistrement")
        except Exception as sample_error:
            print(f"âŒ Erreur lors de la lecture d'Ã©chantillon: {str(sample_error)}")
            print("ğŸ” Erreur dÃ©taillÃ©e:")
            print(str(sample_error))
            
            # Analyser le type d'erreur
            if "No such file or directory" in str(sample_error):
                print("\nğŸš¨ DIAGNOSTIC: Fichiers Parquet manquants dÃ©tectÃ©s!")
                return "missing_files"
            elif "SparkFileNotFoundException" in str(sample_error):
                print("\nğŸš¨ DIAGNOSTIC: Fichiers rÃ©fÃ©rencÃ©s dans les mÃ©tadonnÃ©es mais absents du stockage!")
                return "missing_files"
            else:
                print(f"\nğŸš¨ DIAGNOSTIC: Erreur inconnue: {type(sample_error).__name__}")
                return "unknown_error"
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur gÃ©nÃ©rale lors du diagnostic: {str(e)}")
        return False

def repair_delta_table_missing_files(spark, table_path):
    """RÃ©parer une table Delta avec des fichiers manquants"""
    print(f"\n=== RÃ‰PARATION DE LA TABLE DELTA: {table_path} ===")
    
    try:
        delta_table = DeltaTable.forPath(spark, table_path)
        
        # Option 1: Utiliser une version antÃ©rieure
        print("ğŸ”§ OPTION 1: Restaurer Ã  une version antÃ©rieure")
        try:
            history_df = delta_table.history()
            versions = history_df.select("version").orderBy(desc("version")).collect()
            
            print(f"ğŸ“‹ Versions disponibles: {[row.version for row in versions[:5]]}")
            
            # Essayer les versions prÃ©cÃ©dentes une par une
            for version_row in versions[1:6]:  # Ignorer la version actuelle, essayer les 5 suivantes
                version = version_row.version
                print(f"ğŸ”„ Test de la version {version}...")
                
                try:
                    # Essayer de lire cette version
                    version_df = spark.read.format("delta").option("versionAsOf", version).load(table_path)
                    test_count = version_df.count()
                    print(f"âœ… Version {version} accessible avec {test_count} enregistrements")
                    
                    # Demander si on veut restaurer cette version
                    print(f"ğŸ”„ Restauration Ã  la version {version}...")
                    delta_table.restoreToVersion(version)
                    print(f"âœ… Table restaurÃ©e Ã  la version {version}")
                    return True
                    
                except Exception as version_error:
                    print(f"âŒ Version {version} Ã©galement corrompue: {str(version_error)}")
                    continue
            
            print("âŒ Aucune version antÃ©rieure utilisable trouvÃ©e")
            
        except Exception as restore_error:
            print(f"âŒ Erreur lors de la restauration: {str(restore_error)}")
        
        # Option 2: Reconstruire la table depuis les fichiers existants
        print("\nğŸ”§ OPTION 2: Reconstruire depuis les fichiers Parquet existants")
        try:
            # Lister tous les fichiers Parquet encore existants
            existing_files_df = spark.sql(f"""
                SELECT input_file_name() as file_path, *
                FROM delta.`{table_path}`
                WHERE input_file_name() IS NOT NULL
            """)
            
            print("Cette option nÃ©cessite une intervention manuelle plus complexe.")
            print("Recommandation: Utiliser l'Option 3 (reconstruction complÃ¨te)")
            
        except Exception as rebuild_error:
            print(f"âŒ Erreur lors de la reconstruction: {str(rebuild_error)}")
        
        return False
        
    except Exception as e:
        print(f"âŒ Erreur lors de la rÃ©paration: {str(e)}")
        return False

def clean_and_rebuild_delta_table(spark, table_path, backup_path=None):
    """Nettoyer complÃ¨tement et reconstruire la table Delta"""
    print(f"\n=== RECONSTRUCTION COMPLÃˆTE DE LA TABLE DELTA ===")
    
    try:
        # 1. CrÃ©er une sauvegarde si possible
        if backup_path:
            print(f"ğŸ’¾ Tentative de sauvegarde vers {backup_path}...")
            try:
                delta_table = DeltaTable.forPath(spark, table_path)
                backup_df = delta_table.toDF()
                backup_count = backup_df.count()
                
                backup_df.write.format("delta").mode("overwrite").save(backup_path)
                print(f"âœ… Sauvegarde crÃ©Ã©e: {backup_count} enregistrements dans {backup_path}")
                
            except Exception as backup_error:
                print(f"âš ï¸ Impossible de crÃ©er une sauvegarde: {str(backup_error)}")
                print("ğŸ”„ Continuation sans sauvegarde...")
        
        # 2. Supprimer complÃ¨tement la table Delta actuelle
        print(f"ğŸ—‘ï¸ Suppression de la table Delta corrompue...")
        try:
            # Note: En production, vous devriez utiliser des commandes systÃ¨me pour supprimer
            # Pour MinIO/S3, cela nÃ©cessiterait des appels API spÃ©cifiques
            print("âš ï¸ ATTENTION: Suppression manuelle requise!")
            print(f"ExÃ©cutez manuellement: rm -rf {table_path}")
            print("Ou utilisez l'interface MinIO pour supprimer le dossier")
            
            # Alternative: Essayer de recrÃ©er par-dessus
            print("ğŸ”„ Tentative de recrÃ©ation par Ã©crasement...")
            
        except Exception as delete_error:
            print(f"âš ï¸ Erreur lors de la suppression: {str(delete_error)}")
        
        # 3. Reconstruire depuis les fichiers CSV originaux
        print("ğŸ”„ Pour reconstruire complÃ¨tement:")
        print("1. Supprimez manuellement le dossier de la table Delta")
        print("2. Relancez votre script de conversion CSV vers Delta")
        print("3. Assurez-vous que les fichiers CSV sources sont toujours disponibles")
        
        return False
        
    except Exception as e:
        print(f"âŒ Erreur lors de la reconstruction: {str(e)}")
        return False

def clear_spark_cache_and_refresh(spark, table_path):
    """Nettoyer le cache Spark et rafraÃ®chir les mÃ©tadonnÃ©es"""
    print(f"\n=== NETTOYAGE DU CACHE SPARK ===")
    
    try:
        # 1. Vider tout le cache Spark
        print("ğŸ§¹ Vidage du cache Spark...")
        spark.sql("CLEAR CACHE")
        spark.catalog.clearCache()
        print("âœ… Cache Spark vidÃ©")
        
        # 2. RafraÃ®chir la table spÃ©cifiquement
        print(f"ğŸ”„ RafraÃ®chissement de la table {table_path}...")
        try:
            # Essayer diffÃ©rentes mÃ©thodes de rafraÃ®chissement
            spark.sql(f"REFRESH TABLE delta.`{table_path}`")
            print("âœ… Table rafraÃ®chie via SQL")
        except Exception as refresh_error:
            print(f"âš ï¸ RafraÃ®chissement SQL Ã©chouÃ©: {str(refresh_error)}")
            
            try:
                # Alternative: recrÃ©er le DataFrame
                df = spark.read.format("delta").load(table_path)
                df.cache()
                df.count()  # Force l'Ã©valuation
                print("âœ… DataFrame recrÃ©Ã© et mis en cache")
            except Exception as df_error:
                print(f"âŒ Impossible de recrÃ©er le DataFrame: {str(df_error)}")
        
        # 3. Invalider les mÃ©tadonnÃ©es du catalog
        print("ğŸ”„ Invalidation des mÃ©tadonnÃ©es du catalog...")
        try:
            spark.sql("REFRESH")
            print("âœ… MÃ©tadonnÃ©es du catalog invalidÃ©es")
        except Exception as catalog_error:
            print(f"âš ï¸ Invalidation du catalog Ã©chouÃ©e: {str(catalog_error)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lors du nettoyage du cache: {str(e)}")
        return False

def safe_read_delta_table(spark, table_path):
    """Lecture sÃ©curisÃ©e de la table Delta avec gestion d'erreurs"""
    print(f"=== LECTURE SÃ‰CURISÃ‰E DE LA TABLE DELTA ===")
    
    try:
        # 1. Nettoyer d'abord le cache
        clear_spark_cache_and_refresh(spark, table_path)
        
        # 2. Essayer une lecture progressive
        print("ğŸ” Tentative de lecture progressive...")
        
        # Ã‰tape 1: Lire seulement le schÃ©ma
        try:
            schema_df = spark.read.format("delta").load(table_path).limit(0)
            print(f"âœ… SchÃ©ma lu: {len(schema_df.columns)} colonnes")
        except Exception as schema_error:
            print(f"âŒ Impossible de lire le schÃ©ma: {str(schema_error)}")
            return None
        
        # Ã‰tape 2: Essayer de lire par petits lots
        try:
            print("ğŸ” Lecture par petits Ã©chantillons...")
            
            for limit in [1, 10, 100, 1000]:
                try:
                    sample_df = spark.read.format("delta").load(table_path).limit(limit)
                    count = sample_df.count()
                    print(f"âœ… Lecture rÃ©ussie pour {limit} enregistrements (rÃ©sultat: {count})")
                    
                    if count > 0:
                        # Montrer un Ã©chantillon
                        print("ğŸ“‹ Ã‰chantillon des donnÃ©es:")
                        sample_df.show(5, truncate=True)
                        return sample_df
                    
                except Exception as sample_error:
                    print(f"âŒ Ã‰chec pour {limit} enregistrements: {str(sample_error)}")
                    continue
            
            print("âŒ Toutes les tentatives de lecture par Ã©chantillon ont Ã©chouÃ©")
            
        except Exception as progressive_error:
            print(f"âŒ Erreur lors de la lecture progressive: {str(progressive_error)}")
        
        # Ã‰tape 3: Essayer de lire une partition spÃ©cifique
        try:
            print("ğŸ” Tentative de lecture par partition...")
            
            # Essayer de lire les mÃ©tadonnÃ©es de partitionnement
            delta_table = DeltaTable.forPath(spark, table_path)
            
            # Lire une partition spÃ©cifique qui pourrait Ãªtre valide
            partition_df = spark.read.format("delta").load(table_path) \
                              .filter((col("year") == 2025) & (col("month") == 7) & (col("day") == 29))
            
            partition_count = partition_df.count()
            print(f"âœ… Partition 2025/7/29 contient {partition_count} enregistrements")
            
            if partition_count > 0:
                print("ğŸ“‹ DonnÃ©es de la partition:")
                partition_df.show(5, truncate=True)
                return partition_df
            
        except Exception as partition_error:
            print(f"âŒ Lecture par partition Ã©chouÃ©e: {str(partition_error)}")
        
        return None
        
    except Exception as e:
        print(f"âŒ Erreur lors de la lecture sÃ©curisÃ©e: {str(e)}")
        return None

def main():
    """Fonction principale de diagnostic et rÃ©paration"""
    print("=== OUTIL DE DIAGNOSTIC ET RÃ‰PARATION DELTA LAKE ===")
    
    table_path = "s3a://transactions/bronze"
    backup_path = "s3a://transactions/bronze_backup"
    
    spark = create_spark_session()
    
    try:
        # 1. Diagnostic complet
        diagnosis_result = diagnose_delta_table_issues(spark, table_path)
        
        if diagnosis_result == "missing_files":
            print("\nğŸ”§ PROBLÃˆME DÃ‰TECTÃ‰: Fichiers Parquet manquants")
            print("ğŸ’¡ Solutions possibles:")
            print("1. Restaurer Ã  une version antÃ©rieure")
            print("2. Reconstruire depuis les CSV originaux")
            print("3. Nettoyer le cache et rÃ©essayer")
            
            # Essayer le nettoyage du cache d'abord
            if clear_spark_cache_and_refresh(spark, table_path):
                print("ğŸ”„ Nouvelle tentative aprÃ¨s nettoyage du cache...")
                result_df = safe_read_delta_table(spark, table_path)
                
                if result_df is not None:
                    print("âœ… ProblÃ¨me rÃ©solu par le nettoyage du cache!")
                    return
            
            # Si le cache ne rÃ©sout pas le problÃ¨me, essayer la rÃ©paration
            print("ğŸ”§ Tentative de rÃ©paration...")
            if not repair_delta_table_missing_files(spark, table_path):
                print("âŒ RÃ©paration automatique impossible")
                print("ğŸ’¡ SOLUTION RECOMMANDÃ‰E:")
                print("1. Sauvegardez les fichiers CSV originaux")
                print("2. Supprimez complÃ¨tement le dossier s3a://transactions/bronze")
                print("3. Relancez votre script de conversion CSV vers Delta")
                print("4. VÃ©rifiez que les colonnes de partitionnement ne contiennent pas de valeurs NULL")
                
        elif diagnosis_result == True:
            print("âœ… Table Delta semble fonctionnelle")
            result_df = safe_read_delta_table(spark, table_path)
            
            if result_df is not None:
                print("âœ… Lecture rÃ©ussie!")
            else:
                print("âš ï¸ ProblÃ¨me lors de la lecture malgrÃ© le diagnostic positif")
        
        else:
            print("âŒ Diagnostic a rÃ©vÃ©lÃ© des problÃ¨mes majeurs")
            print("ğŸ’¡ Reconstruction complÃ¨te recommandÃ©e")
        
        # Conseils de prÃ©vention
        print(f"\nğŸ“‹ CONSEILS DE PRÃ‰VENTION:")
        print("1. Toujours valider que les colonnes de partitionnement ne sont pas NULL avant d'Ã©crire")
        print("2. Utiliser des transactions Delta pour les Ã©critures")
        print("3. Faire des sauvegardes rÃ©guliÃ¨res")
        print("4. Utiliser VACUUM avec prÃ©caution (garde l'historique)")
        print("5. Monitorer l'espace disque pour Ã©viter les Ã©critures incomplÃ¨tes")
        
    except Exception as e:
        print(f"âŒ Erreur lors du diagnostic/rÃ©paration: {str(e)}")
        import traceback
        traceback.print_exc()
    
    finally:
        print("ğŸ”š Fermeture de la session Spark...")
        spark.stop()

if __name__ == "__main__":
    main()

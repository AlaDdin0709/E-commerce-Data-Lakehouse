from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta import DeltaTable
import re
from datetime import datetime

def create_spark_session():
    """CrÃ©er une session Spark avec les configurations nÃ©cessaires pour Delta Lake"""
    spark = SparkSession.builder \
        .appName("CustomerImagesBronzeToSilverCleaner") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio-api.minio.svc.cluster.local:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.kubernetes.file.upload.path", "/tmp") \
        .config("spark.kubernetes.executor.deleteOnTermination", "true") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark

def register_delta_tables(spark, bronze_path, silver_path):
    """Enregistrer les tables Delta pour utilisation avec Spark SQL"""
    try:
        # Lire la table Bronze et crÃ©er une vue temporaire
        bronze_df = spark.read.format("delta").load(bronze_path)
        bronze_df.createOrReplaceTempView("bronze_customer_images")
        print("âœ… Table Bronze Customer Images enregistrÃ©e comme vue temporaire")
        
        # VÃ©rifier si la table Silver existe
        if DeltaTable.isDeltaTable(spark, silver_path):
            silver_df = spark.read.format("delta").load(silver_path)
            silver_df.createOrReplaceTempView("silver_customer_images")
            print("âœ… Table Silver Customer Images existante enregistrÃ©e comme vue temporaire")
            return True
        else:
            print("â„¹ï¸ Table Silver Customer Images n'existe pas encore - premiÃ¨re exÃ©cution")
            return False
            
    except Exception as e:
        print(f"âŒ Erreur lors de l'enregistrement des tables: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def get_max_processing_timestamp_silver(spark, silver_exists):
    """Obtenir le timestamp de traitement maximum de la table Silver"""
    if not silver_exists:
        print("ğŸ†• PremiÃ¨re exÃ©cution - traitement de toutes les donnÃ©es Bronze")
        return None
    
    try:
        max_timestamp_sql = """
            SELECT MAX(processing_timestamp) as max_timestamp
            FROM silver_customer_images
        """
        
        result = spark.sql(max_timestamp_sql).collect()
        
        if result and result[0].max_timestamp:
            max_timestamp = result[0].max_timestamp
            print(f"ğŸ“… Dernier timestamp traitÃ© en Silver: {max_timestamp}")
            return max_timestamp
        else:
            print("â„¹ï¸ Aucun timestamp trouvÃ© en Silver - traitement complet")
            return None
            
    except Exception as e:
        print(f"âš ï¸ Erreur lors de la rÃ©cupÃ©ration du max timestamp: {str(e)}")
        return None

def check_image_id_uniqueness(spark):
    """VÃ©rifier l'unicitÃ© des image_id dans les nouvelles donnÃ©es et compter les doublons Ã  Ã©liminer"""
    try:
        duplicates_sql = """
            SELECT image_id, COUNT(*) as count
            FROM bronze_customer_images
            WHERE image_id IS NOT NULL AND TRIM(image_id) != ''
            GROUP BY image_id
            HAVING COUNT(*) > 1
            ORDER BY count DESC
            LIMIT 10
        """
        
        duplicates_df = spark.sql(duplicates_sql)
        duplicate_count = duplicates_df.count()
        
        if duplicate_count > 0:
            print(f"âš ï¸ ATTENTION: {duplicate_count} image_id avec doublons trouvÃ©s (seul le plus rÃ©cent sera conservÃ©):")
            duplicates_df.show(10, truncate=False)
            
            # Calculer le nombre total de doublons Ã  Ã©liminer
            total_duplicates_sql = """
                SELECT SUM(count - 1) as total_to_remove
                FROM (
                    SELECT image_id, COUNT(*) as count
                    FROM bronze_customer_images
                    WHERE image_id IS NOT NULL AND TRIM(image_id) != ''
                    GROUP BY image_id
                    HAVING COUNT(*) > 1
                ) duplicates
            """
            
            result = spark.sql(total_duplicates_sql).collect()
            if result and result[0].total_to_remove:
                total_to_remove = result[0].total_to_remove
                print(f"ğŸ—‘ï¸ {total_to_remove} enregistrements dupliquÃ©s seront Ã©liminÃ©s")
            
        else:
            print("âœ… Aucun image_id dupliquÃ© trouvÃ©")
            
        return duplicate_count
        
    except Exception as e:
        print(f"âš ï¸ Erreur lors de la vÃ©rification des doublons: {str(e)}")
        return 0

def create_silver_table_sql(spark, silver_path, max_timestamp_silver):
    """CrÃ©er la requÃªte SQL pour nettoyer et transformer les donnÃ©es customer images avec dÃ©duplication"""
    
    # Condition pour le traitement incrÃ©mental
    incremental_condition = ""
    if max_timestamp_silver:
        incremental_condition = f"AND processing_timestamp > '{max_timestamp_silver}'"
    
    # RequÃªte SQL avec dÃ©duplication par image_id (garde le plus rÃ©cent)
    cleaning_sql = f"""
        WITH ranked_data AS (
            SELECT 
                -- Nettoyage et validation du image_id (clÃ© unique)
                CASE 
                    WHEN image_id IS NOT NULL AND TRIM(image_id) != '' 
                    THEN TRIM(image_id)
                    ELSE NULL 
                END as image_id,
                
                -- Nettoyage du s3_path - accepter les chemins relatifs aussi
                CASE 
                    WHEN s3_path IS NOT NULL AND TRIM(s3_path) != '' 
                    THEN TRIM(s3_path)
                    ELSE NULL 
                END as s3_path,
                
                -- Validation du customer_id
                CASE 
                    WHEN customer_id IS NOT NULL AND TRIM(customer_id) != '' 
                    THEN TRIM(customer_id)
                    ELSE NULL 
                END as customer_id,
                
                -- Utiliser order_id s'il existe, sinon extracted_order_id
                CASE 
                    WHEN order_id IS NOT NULL AND TRIM(order_id) != '' 
                    THEN TRIM(order_id)
                    WHEN extracted_order_id IS NOT NULL AND TRIM(extracted_order_id) != '' 
                    THEN TRIM(extracted_order_id)
                    ELSE NULL 
                END as order_id,
                
                processing_timestamp,
                upload_timestamp,
                partition_year,
                partition_month,
                partition_day,
                current_timestamp() as silver_load_timestamp,
                
                -- Ranger par processing_timestamp DESC pour garder le plus rÃ©cent
                ROW_NUMBER() OVER (
                    PARTITION BY TRIM(image_id) 
                    ORDER BY processing_timestamp DESC
                ) as rn
                
            FROM bronze_customer_images
            WHERE image_id IS NOT NULL
                AND TRIM(image_id) != ''
                AND processing_timestamp IS NOT NULL
                AND partition_year IS NOT NULL
                AND partition_month IS NOT NULL
                AND partition_day IS NOT NULL
                {incremental_condition}
        )
        SELECT 
            image_id,
            s3_path,
            customer_id,
            order_id,
            processing_timestamp,
            upload_timestamp,
            partition_year,
            partition_month,
            partition_day,
            silver_load_timestamp
        FROM ranked_data
        WHERE rn = 1
    """
    
    return cleaning_sql

def execute_bronze_to_silver_transformation(spark, bronze_path, silver_path):
    """ExÃ©cuter la transformation complÃ¨te Bronze vers Silver pour Customer Images"""
    
    print("=== DÃ‰MARRAGE DE LA TRANSFORMATION BRONZE VERS SILVER CUSTOMER IMAGES ===")
    
    # 1. Enregistrer les tables Delta
    silver_exists = register_delta_tables(spark, bronze_path, silver_path)
    
    # 2. VÃ©rifier l'unicitÃ© des image_id
    duplicate_count = check_image_id_uniqueness(spark)
    
    # 3. Obtenir le timestamp maximum de Silver pour traitement incrÃ©mental
    max_timestamp_silver = get_max_processing_timestamp_silver(spark, silver_exists)
    
    # 4. CrÃ©er la requÃªte SQL de nettoyage
    cleaning_sql = create_silver_table_sql(spark, silver_path, max_timestamp_silver)
    
    print("ğŸ“‹ RequÃªte SQL de transformation crÃ©Ã©e")
    print("ğŸ” VÃ©rification des nouvelles donnÃ©es Ã  traiter...")
    
    # 5. ExÃ©cuter la requÃªte et compter les rÃ©sultats
    try:
        cleaned_df = spark.sql(cleaning_sql)
        new_records_count = cleaned_df.count()
        
        if new_records_count == 0:
            print("âœ… Aucune nouvelle donnÃ©e Ã  traiter - Silver est Ã  jour")
            return
        
        print(f"ğŸ“Š {new_records_count} nouveaux enregistrements Ã  traiter")
        
        # 6. VÃ©rifier l'unicitÃ© des image_id dans les donnÃ©es nettoyÃ©es
        print("\nğŸ” VÃ‰RIFICATION DE L'UNICITÃ‰ DES IMAGE_ID APRÃˆS NETTOYAGE:")
        unique_images_sql = """
            SELECT COUNT(DISTINCT image_id) as unique_images,
                   COUNT(*) as total_records
            FROM (
                SELECT * FROM (""" + cleaning_sql + """) as cleaned_data
            ) t
        """
        
        unique_result = spark.sql(unique_images_sql).collect()[0]
        unique_images = unique_result.unique_images
        total_records = unique_result.total_records
        
        print(f"ğŸ“Š Images uniques: {unique_images}")
        print(f"ğŸ“Š Total enregistrements: {total_records}")
        
        if unique_images == total_records:
            print("âœ… Tous les image_id sont maintenant uniques aprÃ¨s dÃ©duplication")
        else:
            print(f"âŒ ERREUR: DÃ©duplication Ã©chouÃ©e - {total_records - unique_images} doublons restants")
        
        # 7. Afficher un Ã©chantillon des donnÃ©es nettoyÃ©es
        print("\nğŸ” Ã‰CHANTILLON DES DONNÃ‰ES NETTOYÃ‰ES:")
        sample_df = cleaned_df.limit(5)
        sample_df.select("image_id", "s3_path", "customer_id", "order_id").show(truncate=False)
        
        # 8. VÃ©rifier la qualitÃ© des donnÃ©es nettoyÃ©es
        print("\nğŸ“Š QUALITÃ‰ DES DONNÃ‰ES APRÃˆS NETTOYAGE:")
        
        # Compter les valeurs nulles pour les champs critiques
        null_image_id = cleaned_df.filter(col("image_id").isNull()).count()
        null_s3_path = cleaned_df.filter(col("s3_path").isNull()).count()
        null_customer_id = cleaned_df.filter(col("customer_id").isNull()).count()
        null_order_id = cleaned_df.filter(col("order_id").isNull()).count()
        
        print(f"ğŸ–¼ï¸ Image ID manquants/invalides: {null_image_id}/{new_records_count}")
        print(f"ğŸ“ Chemins S3 manquants/invalides: {null_s3_path}/{new_records_count}")
        print(f"ğŸ‘¤ Customer ID manquants/invalides: {null_customer_id}/{new_records_count}")
        print(f"ğŸ“¦ Order ID manquants/invalides: {null_order_id}/{new_records_count}")
        
        # 9. Ã‰crire les donnÃ©es dans la table Silver
        print(f"\nğŸ’¾ Ã‰criture de {new_records_count} enregistrements vers Silver...")
        
        if silver_exists:
            print("ğŸ“Š Mode APPEND - Ajout des nouvelles donnÃ©es")
            
            cleaned_df.write \
                .format("delta") \
                .mode("append") \
                .partitionBy("partition_year", "partition_month", "partition_day") \
                .option("mergeSchema", "true") \
                .save(silver_path)
        else:
            print("ğŸ†• Mode OVERWRITE - CrÃ©ation de la nouvelle table Silver")
            
            cleaned_df.write \
                .format("delta") \
                .mode("overwrite") \
                .partitionBy("partition_year", "partition_month", "partition_day") \
                .option("delta.autoOptimize.optimizeWrite", "true") \
                .option("delta.autoOptimize.autoCompact", "true") \
                .save(silver_path)
        
        # 10. VÃ©rification finale
        print("\nâœ… TRANSFORMATION TERMINÃ‰E AVEC SUCCÃˆS")
        
        # Comptage final et vÃ©rification des image_id uniques en Silver
        silver_final_df = spark.read.format("delta").load(silver_path)
        total_silver_count = silver_final_df.count()
        
        # VÃ©rifier l'unicitÃ© finale en Silver
        silver_final_df.createOrReplaceTempView("final_silver")
        final_unique_sql = """
            SELECT COUNT(DISTINCT image_id) as unique_images,
                   COUNT(*) as total_records
            FROM final_silver
        """
        
        final_result = spark.sql(final_unique_sql).collect()[0]
        final_unique_images = final_result.unique_images
        final_total_records = final_result.total_records
        
        print(f"ğŸ“ˆ Total des enregistrements en Silver: {total_silver_count:,}")
        print(f"ğŸ”‘ Images uniques en Silver: {final_unique_images:,}")
        
        if final_unique_images == final_total_records:
            print("âœ… Tous les image_id sont uniques en Silver - DÃ©duplication rÃ©ussie!")
        else:
            print(f"âŒ ERREUR: {final_total_records - final_unique_images} doublons prÃ©sents en Silver")
        
        # 11. Statistiques business
        print("\nğŸ“Š STATISTIQUES BUSINESS CUSTOMER IMAGES:")
        
        # VÃ©rification des s3_path non-null
        s3_non_null_sql = """
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN s3_path IS NOT NULL THEN 1 ELSE 0 END) as s3_path_present,
                SUM(CASE WHEN s3_path IS NULL THEN 1 ELSE 0 END) as s3_path_missing
            FROM final_silver
        """
        
        s3_stats = spark.sql(s3_non_null_sql).collect()[0]
        print(f"ğŸ“ S3 Paths prÃ©sents: {s3_stats.s3_path_present}/{s3_stats.total}")
        print(f"ğŸ“ S3 Paths manquants: {s3_stats.s3_path_missing}/{s3_stats.total}")
        
        # Nombre de clients uniques
        customer_stats_sql = """
            SELECT COUNT(DISTINCT customer_id) as unique_customers
            FROM final_silver
            WHERE customer_id IS NOT NULL
        """
        customer_result = spark.sql(customer_stats_sql).collect()[0]
        print(f"ğŸ‘¥ Clients uniques: {customer_result.unique_customers}")
        
        # Nombre de commandes uniques
        order_stats_sql = """
            SELECT COUNT(DISTINCT order_id) as unique_orders
            FROM final_silver
            WHERE order_id IS NOT NULL
        """
        order_result = spark.sql(order_stats_sql).collect()[0]
        print(f"ğŸ“¦ Commandes uniques: {order_result.unique_orders}")
        
        print(f"\nğŸ‰ TRANSFORMATION BRONZE -> SILVER CUSTOMER IMAGES COMPLÃ‰TÃ‰E!")
        print(f"ğŸ“ Table Silver disponible Ã : {silver_path}")
        print(f"ğŸ“Š {new_records_count:,} nouveaux enregistrements ajoutÃ©s")
        print(f"ğŸ“ˆ Total des enregistrements: {total_silver_count:,}")
        print(f"ğŸ”‘ Images uniques: {final_unique_images:,}")
        
        # RÃ©sumÃ© des colonnes conservÃ©es
        print(f"\nğŸ“‹ COLONNES CONSERVÃ‰ES EN SILVER:")
        print("   1. image_id (unique identifier)")
        print("   2. s3_path")
        print("   3. customer_id")
        print("   4. order_id")
        print("   5. processing_timestamp")
        print("   6. upload_timestamp")
        print("   7. partition_year")
        print("   8. partition_month")
        print("   9. partition_day")
        print("  10. silver_load_timestamp")
        
    except Exception as e:
        print(f"âŒ Erreur lors de la transformation: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e

def main():
    """Fonction principale"""
    print("=== NETTOYAGE BRONZE VERS SILVER CUSTOMER IMAGES AVEC SPARK SQL ===")
    
    spark = create_spark_session()
    
    try:
        # Chemins des tables
        bronze_path = "s3a://customer-images/bronze"
        silver_path = "s3a://customer-images/silver"
        
        # VÃ©rifier que la table Bronze existe
        if not DeltaTable.isDeltaTable(spark, bronze_path):
            print(f"âŒ Table Bronze non trouvÃ©e Ã : {bronze_path}")
            return
        
        print(f"âœ… Table Bronze trouvÃ©e Ã : {bronze_path}")
        
        # ExÃ©cuter la transformation
        execute_bronze_to_silver_transformation(spark, bronze_path, silver_path)
        
    except Exception as e:
        print(f"âŒ Erreur dans le processus principal: {str(e)}")
        raise e
    finally:
        print("ğŸ”š Fermeture de la session Spark...")
        spark.stop()

if __name__ == "__main__":
    main()

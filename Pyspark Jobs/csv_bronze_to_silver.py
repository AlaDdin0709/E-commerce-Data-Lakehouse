from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta import DeltaTable
import re
from datetime import datetime

def create_spark_session():
    """CrÃ©er une session Spark avec les configurations nÃ©cessaires pour Delta Lake"""
    spark = SparkSession.builder \
        .appName("BronzeToSilverCleaner") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio-api.minio.svc.cluster.local:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.kubernetes.file.upload.path", "/tmp") \
        .config("spark.kubernetes.executor.deleteOnTermination", "true") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark

def register_delta_tables(spark, bronze_path, silver_path):
    """Enregistrer les tables Delta pour utilisation avec Spark SQL"""
    try:
        # MÃ©thode alternative pour enregistrer la table Bronze
        # Lire la table Delta et crÃ©er une vue temporaire
        bronze_df = spark.read.format("delta").load(bronze_path)
        bronze_df.createOrReplaceTempView("bronze_transactions")
        print("âœ… Table Bronze enregistrÃ©e comme vue temporaire")
        
        # VÃ©rifier si la table Silver existe
        if DeltaTable.isDeltaTable(spark, silver_path):
            silver_df = spark.read.format("delta").load(silver_path)
            silver_df.createOrReplaceTempView("silver_transactions")
            print("âœ… Table Silver existante enregistrÃ©e comme vue temporaire")
            return True
        else:
            print("â„¹ï¸ Table Silver n'existe pas encore - premiÃ¨re exÃ©cution")
            return False
            
    except Exception as e:
        print(f"âŒ Erreur lors de l'enregistrement des tables: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def get_max_processing_timestamp_silver(spark, silver_exists):
    """Obtenir le timestamp de traitement maximum de la table Silver"""
    if not silver_exists:
        print("ğŸ†• PremiÃ¨re exÃ©cution - traitement de toutes les donnÃ©es Bronze")
        return None
    
    try:
        max_timestamp_sql = """
            SELECT MAX(processing_timestamp) as max_timestamp
            FROM silver_transactions
        """
        
        result = spark.sql(max_timestamp_sql).collect()
        
        if result and result[0].max_timestamp:
            max_timestamp = result[0].max_timestamp
            print(f"ğŸ“… Dernier timestamp traitÃ© en Silver: {max_timestamp}")
            return max_timestamp
        else:
            print("â„¹ï¸ Aucun timestamp trouvÃ© en Silver - traitement complet")
            return None
            
    except Exception as e:
        print(f"âš ï¸ Erreur lors de la rÃ©cupÃ©ration du max timestamp: {str(e)}")
        return None

def create_cleaning_functions(spark):
    """CrÃ©er les UDFs pour le nettoyage des donnÃ©es"""
    
    def clean_city_udf(shipping_address):
        """Nettoyer et extraire la ville de shipping_address"""
        if not shipping_address:
            return None
        
        try:
            # Patterns pour extraire la ville
            patterns = [
                r'"city":\s*"([^"]+)"',  # "city": "Toronto"
                r'\\city\\":\s*\\"([^\\]+)\\"',  # \"city\": \"Toronto\"
                r'city.*?:\s*["\']([^"\']+)["\']',  # Variations gÃ©nÃ©riques
            ]
            
            for pattern in patterns:
                match = re.search(pattern, shipping_address, re.IGNORECASE)
                if match:
                    city = match.group(1)
                    # Nettoyer les Ã©chappements Unicode
                    city = city.encode().decode('unicode_escape') if '\\u' in city else city
                    return city.strip()
            
            # Si aucun pattern ne marche, essayer d'extraire tout ce qui ressemble Ã  un nom de ville
            clean_text = re.sub(r'[{}"\\\']', '', shipping_address)
            clean_text = re.sub(r'city\s*:', '', clean_text, flags=re.IGNORECASE)
            clean_text = clean_text.strip()
            
            if len(clean_text) > 0 and len(clean_text) < 50:  # Limite raisonnable
                return clean_text
                
            return None
            
        except Exception:
            return None
    
    def clean_region_udf(timestamp_raw):
        """Nettoyer et extraire la rÃ©gion de timestamp_raw"""
        if not timestamp_raw:
            return None
            
        try:
            # Patterns pour extraire la rÃ©gion
            patterns = [
                r'"region":\s*"([^"]+)"',  # "region": "Ontario"
                r'\\region\\":\s*\\"([^\\]+)\\"',  # \"region\": \"Ontario\"
                r'region.*?:\s*["\']([^"\']+)["\']',  # Variations gÃ©nÃ©riques
            ]
            
            for pattern in patterns:
                match = re.search(pattern, timestamp_raw, re.IGNORECASE)
                if match:
                    region = match.group(1)
                    # Nettoyer les Ã©chappements Unicode
                    region = region.encode().decode('unicode_escape') if '\\u' in region else region
                    return region.strip()
            
            # Si aucun pattern ne marche
            clean_text = re.sub(r'[{}"\\\']', '', timestamp_raw)
            clean_text = re.sub(r'region\s*:', '', clean_text, flags=re.IGNORECASE)
            clean_text = clean_text.strip()
            
            if len(clean_text) > 0 and len(clean_text) < 100:  # Limite raisonnable
                return clean_text
                
            return None
            
        except Exception:
            return None
    
    # Enregistrer les UDFs
    spark.udf.register("clean_city", clean_city_udf, StringType())
    spark.udf.register("clean_region", clean_region_udf, StringType())
    
    print("âœ… Fonctions de nettoyage UDF enregistrÃ©es")

def create_silver_table_sql(spark, silver_path, max_timestamp_silver):
    """CrÃ©er la requÃªte SQL pour nettoyer et transformer les donnÃ©es"""
    
    # Condition pour le traitement incrÃ©mental
    incremental_condition = ""
    if max_timestamp_silver:
        incremental_condition = f"AND processing_timestamp > '{max_timestamp_silver}'"
    
    cleaning_sql = f"""
        SELECT 
            order_id,
            customer_id,
            customer_first_name,
            customer_last_name,
            product_id,
            product_name,
            category,
            CAST(amount AS DOUBLE) as amount,
            payment_method,
            payment_status,
            discount_code,
            clean_city(shipping_address) as city,
            clean_region(timestamp_raw) as region,
            CASE 
                WHEN LOWER(TRIM(is_returned)) IN ('true', '1', 'yes', 't') THEN true
                WHEN LOWER(TRIM(is_returned)) IN ('false', '0', 'no', 'f') THEN false
                ELSE false
            END as is_returned,
            processing_timestamp,
            year,
            month,
            day,
            current_timestamp() as silver_load_timestamp
        FROM bronze_transactions
        WHERE order_id IS NOT NULL 
            AND customer_id IS NOT NULL
            AND product_id IS NOT NULL
            AND amount IS NOT NULL
            AND processing_timestamp IS NOT NULL
            {incremental_condition}
    """
    
    return cleaning_sql

def execute_bronze_to_silver_transformation(spark, bronze_path, silver_path):
    """ExÃ©cuter la transformation complÃ¨te Bronze vers Silver"""
    
    print("=== DÃ‰MARRAGE DE LA TRANSFORMATION BRONZE VERS SILVER ===")
    
    # 1. Enregistrer les tables Delta
    silver_exists = register_delta_tables(spark, bronze_path, silver_path)
    
    # 2. Obtenir le timestamp maximum de Silver pour traitement incrÃ©mental
    max_timestamp_silver = get_max_processing_timestamp_silver(spark, silver_exists)
    
    # 3. CrÃ©er les fonctions de nettoyage
    create_cleaning_functions(spark)
    
    # 4. CrÃ©er la requÃªte SQL de nettoyage
    cleaning_sql = create_silver_table_sql(spark, silver_path, max_timestamp_silver)
    
    print("ğŸ“‹ RequÃªte SQL de transformation crÃ©Ã©e")
    print("ğŸ” VÃ©rification des nouvelles donnÃ©es Ã  traiter...")
    
    # 5. ExÃ©cuter la requÃªte et compter les rÃ©sultats
    try:
        cleaned_df = spark.sql(cleaning_sql)
        new_records_count = cleaned_df.count()
        
        if new_records_count == 0:
            print("âœ… Aucune nouvelle donnÃ©e Ã  traiter - Silver est Ã  jour")
            return
        
        print(f"ğŸ“Š {new_records_count} nouveaux enregistrements Ã  traiter")
        
        # 6. Afficher un Ã©chantillon des donnÃ©es nettoyÃ©es
        print("\nğŸ” Ã‰CHANTILLON DES DONNÃ‰ES NETTOYÃ‰ES:")
        sample_df = cleaned_df.limit(3)
        sample_df.select("order_id", "city", "region", "is_returned", "amount").show(truncate=False)
        
        # 7. VÃ©rifier la qualitÃ© des donnÃ©es nettoyÃ©es
        print("\nğŸ“Š QUALITÃ‰ DES DONNÃ‰ES APRÃˆS NETTOYAGE:")
        
        # Compter les valeurs nulles pour les champs critiques
        null_cities = cleaned_df.filter(col("city").isNull()).count()
        null_regions = cleaned_df.filter(col("region").isNull()).count()
        
        print(f"ğŸ™ï¸ Villes manquantes: {null_cities}/{new_records_count}")
        print(f"ğŸŒ RÃ©gions manquantes: {null_regions}/{new_records_count}")
        
        # Afficher quelques exemples de transformation
        print("\nğŸ§¹ EXEMPLES DE TRANSFORMATION:")
        transformation_examples = spark.sql(f"""
            SELECT 
                shipping_address,
                timestamp_raw,
                clean_city(shipping_address) as city,
                clean_region(timestamp_raw) as region
            FROM bronze_transactions
            WHERE shipping_address IS NOT NULL 
                AND timestamp_raw IS NOT NULL
                {f"AND processing_timestamp > '{max_timestamp_silver}'" if max_timestamp_silver else ""}
            LIMIT 5
        """)
        transformation_examples.show(truncate=False)
        
        # 8. Ã‰crire les donnÃ©es dans la table Silver
        print(f"\nğŸ’¾ Ã‰criture de {new_records_count} enregistrements vers Silver...")
        
        if silver_exists:
            print("ğŸ“Š Mode APPEND - Ajout des nouvelles donnÃ©es")
            
            cleaned_df.write \
                .format("delta") \
                .mode("append") \
                .partitionBy("year", "month", "day") \
                .option("mergeSchema", "true") \
                .save(silver_path)
        else:
            print("ğŸ†• Mode OVERWRITE - CrÃ©ation de la nouvelle table Silver")
            
            cleaned_df.write \
                .format("delta") \
                .mode("overwrite") \
                .partitionBy("year", "month", "day") \
                .option("delta.autoOptimize.optimizeWrite", "true") \
                .option("delta.autoOptimize.autoCompact", "true") \
                .save(silver_path)
        
        # 9. VÃ©rification finale
        print("\nâœ… TRANSFORMATION TERMINÃ‰E AVEC SUCCÃˆS")
        
        # Enregistrer la nouvelle table Silver pour les statistiques finales
        silver_final_df = spark.read.format("delta").load(silver_path)
        silver_final_df.createOrReplaceTempView("silver_transactions_final")
        
        # Statistiques finales
        total_silver_count = spark.sql("SELECT COUNT(*) as count FROM silver_transactions_final").collect()[0].count
        print(f"ğŸ“ˆ Total des enregistrements en Silver: {total_silver_count}")
        
        # RÃ©partition par partition
        print("\nğŸ“ RÃ‰PARTITION PAR PARTITION:")
        partition_stats = spark.sql("""
            SELECT year, month, day, COUNT(*) as count
            FROM silver_transactions_final
            GROUP BY year, month, day
            ORDER BY year DESC, month DESC, day DESC
        """)
        partition_stats.show()
        
        # Quelques statistiques business
        print("\nğŸ“Š STATISTIQUES BUSINESS:")
        
        # RÃ©partition par statut de paiement
        payment_stats = spark.sql("""
            SELECT payment_status, COUNT(*) as count
            FROM silver_transactions_final
            GROUP BY payment_status
            ORDER BY count DESC
        """)
        print("ğŸ’³ RÃ©partition par statut de paiement:")
        payment_stats.show()
        
        # Top 10 des villes
        city_stats = spark.sql("""
            SELECT city, COUNT(*) as count
            FROM silver_transactions_final
            WHERE city IS NOT NULL
            GROUP BY city
            ORDER BY count DESC
            LIMIT 10
        """)
        print("ğŸ™ï¸ Top 10 des villes:")
        city_stats.show()
        
        # Taux de retour
        return_stats = spark.sql("""
            SELECT 
                is_returned,
                COUNT(*) as count,
                ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage
            FROM silver_transactions_final
            GROUP BY is_returned
        """)
        print("ğŸ”„ Taux de retour:")
        return_stats.show()
        
        # 10. Optimiser la table Silver
        print("\nğŸ”§ Optimisation de la table Silver...")
        try:
            spark.sql(f"OPTIMIZE delta.`{silver_path}`")
            print("âœ… Table Silver optimisÃ©e")
        except Exception as opt_error:
            print(f"âš ï¸ Optimisation Ã©chouÃ©e: {str(opt_error)}")
        
        print(f"\nğŸ‰ TRANSFORMATION BRONZE -> SILVER COMPLÃ‰TÃ‰E!")
        print(f"ğŸ“ Table Silver disponible Ã : {silver_path}")
        print(f"ğŸ“Š {new_records_count} nouveaux enregistrements ajoutÃ©s")
        print(f"ğŸ“ˆ Total des enregistrements: {total_silver_count}")
        
    except Exception as e:
        print(f"âŒ Erreur lors de la transformation: {str(e)}")
        import traceback
        traceback.print_exc()
        raise e

def main():
    """Fonction principale"""
    print("=== NETTOYAGE BRONZE VERS SILVER AVEC SPARK SQL ===")
    
    spark = create_spark_session()
    
    try:
        # Chemins des tables
        bronze_path = "s3a://transactions/bronze"
        silver_path = "s3a://transactions/silver"
        
        # VÃ©rifier que la table Bronze existe
        if not DeltaTable.isDeltaTable(spark, bronze_path):
            print(f"âŒ Table Bronze non trouvÃ©e Ã : {bronze_path}")
            return
        
        print(f"âœ… Table Bronze trouvÃ©e Ã : {bronze_path}")
        
        # ExÃ©cuter la transformation
        execute_bronze_to_silver_transformation(spark, bronze_path, silver_path)
        
    except Exception as e:
        print(f"âŒ Erreur dans le processus principal: {str(e)}")
        raise e
    finally:
        print("ğŸ”š Fermeture de la session Spark...")
        spark.stop()

if __name__ == "__main__":
    main()
